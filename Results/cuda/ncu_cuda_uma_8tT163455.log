
*************************************************************
Welcome to SWE

SWE Copyright (C) 2012-2022
  Technische Universitaet Muenchen
  Department of Informatics
  Chair of Scientific Computing
  http://www5.in.tum.de/SWE

SWE comes with ABSOLUTELY NO WARRANTY.
SWE is a free software, and you are welcome
to redistribute it under certain conditions.
Details can be found in the file 'LICENSE'.
*************************************************************
Thu Feb  2 16:34:56 2023	Number of MPI processes: 1
Thu Feb  2 16:34:56 2023	Number of Cells: 64 * 64 = 4096
Thu Feb  2 16:34:56 2023	Number of Blocks: 1 * 1 = 1
Thu Feb  2 16:34:56 2023	Process 0 - Number of Cells: 64 * 64 = 4096
Thu Feb  2 16:34:56 2023	Cell size: 15.625m * 15.625m = 244.141m^2
==PROF== Connected to process 2375701 (/u/home/ge96daf/SWE/build/SWE-MPI-Runner)

Thu Feb  2 16:34:56 2023	Process 0 - Printing device informationThu Feb  2 16:34:56 2023	Process 0 - Current CUDA device (relative to host): 0 ( 4 in total)
Thu Feb  2 16:34:56 2023	Process 0 - CUDA device properties: NVIDIA GeForce RTX 3080 (name), 11040/11040 (driver/runtime version), 8.6 (compute capability)
Thu Feb  2 16:34:56 2023	Connecting SWE blocks at left boundaries.
Thu Feb  2 16:34:56 2023	Connecting SWE blocks at right boundaries.
Thu Feb  2 16:34:56 2023	Connecting SWE blocks at bottom boundaries.
Thu Feb  2 16:34:56 2023	Connecting SWE blocks at top boundaries.
Thu Feb  2 16:34:56 2023	Process 0 - Neighbors: -2 (left), -2 (right), -2 (bottom), -2 (top)
Thu Feb  2 16:34:56 2023	Writing output file at time: 0 seconds
Time left: 99999999 sec (  0% done) [>                                                                                                                                            ] |                                                                                                                                                                                     
------------------------------------------------------------------
Thu Feb  2 16:34:56 2023	Everything is set up, starting the simulation.
------------------------------------------------------------------
Time left: 99999999 sec (  0% done) [>                                                                                                                                            ] /==PROF== Profiling "kernelHdBufferEdges" - 0: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 1: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 2: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelLeftBoundary" - 3: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelRightBoundary" - 4: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelBottomBoundary" - 5: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelTopBoundary" - 6: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 7: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 8: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 9: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 10: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 11: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 12: 0%....50%....100% - 10 passes
==PROF== Profiling "updateUnknownsKernel" - 13: 0%....50%....100% - 10 passes
                                                                                                                                                                                     Thu Feb  2 16:34:58 2023	[0]: Simulation with max. global dt 0.515229 at time: 0 seconds.
Time left:  56.2266 sec (  3% done) [====>                                                                                                                                        ] -==PROF== Profiling "kernelHdBufferEdges" - 14: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 15: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 16: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelLeftBoundary" - 17: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelRightBoundary" - 18: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelBottomBoundary" - 19: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelTopBoundary" - 20: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 21: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 22: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 23: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 24: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 25: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 26: 0%....50%....100% - 10 passes
==PROF== Profiling "updateUnknownsKernel" - 27: 0%....50%....100% - 10 passes
                                                                                                                                                                                     Thu Feb  2 16:34:59 2023	[1]: Simulation with max. global dt 0.499160 at time: 0.515229 seconds.
Time left:  41.3617 sec (  6% done) [=========>                                                                                                                                   ] \                                                                                                                                                                                     Thu Feb  2 16:34:59 2023	Writing output file at time: 1.01439 seconds
Time left:  41.3617 sec (  6% done) [=========>                                                                                                                                   ] |==PROF== Profiling "kernelHdBufferEdges" - 28: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 29: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 30: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelLeftBoundary" - 31: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelRightBoundary" - 32: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelBottomBoundary" - 33: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelTopBoundary" - 34: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 35: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 36: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 37: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 38: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 39: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 40: 0%....50%....100% - 10 passes
==PROF== Profiling "updateUnknownsKernel" - 41: 0%....50%....100% - 10 passes
                                                                                                                                                                                     Thu Feb  2 16:35:00 2023	[2]: Simulation with max. global dt 0.490461 at time: 1.01439 seconds.
Time left:  35.8711 sec ( 10% done) [==============>                                                                                                                              ] /                                                                                                                                                                                     Thu Feb  2 16:35:00 2023	Writing output file at time: 1.50485 seconds
Time left:  35.8711 sec ( 10% done) [==============>                                                                                                                              ] -==PROF== Profiling "kernelHdBufferEdges" - 42: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 43: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 44: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelLeftBoundary" - 45: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelRightBoundary" - 46: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelBottomBoundary" - 47: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelTopBoundary" - 48: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 49: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 50: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 51: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 52: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 53: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 54: 0%....50%....100% - 10 passes
==PROF== Profiling "updateUnknownsKernel" - 55: 0%....50%....100% - 10 passes
                                                                                                                                                                                     Thu Feb  2 16:35:01 2023	[3]: Simulation with max. global dt 0.486515 at time: 1.50485 seconds.
Time left:  32.6626 sec ( 13% done) [==================>                                                                                                                          ] \==PROF== Profiling "kernelHdBufferEdges" - 56: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 57: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 58: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelLeftBoundary" - 59: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelRightBoundary" - 60: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelBottomBoundary" - 61: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelTopBoundary" - 62: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 63: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 64: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 65: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 66: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 67: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 68: 0%....50%....100% - 10 passes
==PROF== Profiling "updateUnknownsKernel" - 69: 0%....50%....100% - 10 passes
                                                                                                                                                                                     Thu Feb  2 16:35:03 2023	[4]: Simulation with max. global dt 0.484222 at time: 1.99136 seconds.
Time left:  35.4142 sec ( 16% done) [=======================>                                                                                                                     ] |                                                                                                                                                                                     Thu Feb  2 16:35:03 2023	Writing output file at time: 2.47559 seconds
Time left:  35.4142 sec ( 16% done) [=======================>                                                                                                                     ] /==PROF== Profiling "kernelHdBufferEdges" - 70: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 71: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 72: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelLeftBoundary" - 73: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelRightBoundary" - 74: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelBottomBoundary" - 75: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelTopBoundary" - 76: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 77: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 78: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 79: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 80: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 81: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 82: 0%....50%....100% - 10 passes
==PROF== Profiling "updateUnknownsKernel" - 83: 0%....50%....100% - 10 passes
                                                                                                                                                                                     Thu Feb  2 16:35:04 2023	[5]: Simulation with max. global dt 0.482701 at time: 2.47559 seconds.
Time left:  32.5640 sec ( 19% done) [===========================>                                                                                                                 ] -==PROF== Profiling "kernelHdBufferEdges" - 84: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 85: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 86: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelLeftBoundary" - 87: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelRightBoundary" - 88: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelBottomBoundary" - 89: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelTopBoundary" - 90: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 91: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 92: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 93: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 94: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 95: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 96: 0%....50%....100% - 10 passes
==PROF== Profiling "updateUnknownsKernel" - 97: 0%....50%....100% - 10 passes
                                                                                                                                                                                     Thu Feb  2 16:35:05 2023	[6]: Simulation with max. global dt 0.481660 at time: 2.95829 seconds.
Time left:  30.2448 sec ( 22% done) [================================>                                                                                                            ] \                                                                                                                                                                                     Thu Feb  2 16:35:05 2023	Writing output file at time: 3.43995 seconds
Time left:  30.2448 sec ( 22% done) [================================>                                                                                                            ] |==PROF== Profiling "kernelHdBufferEdges" - 98: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 99: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 100: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelLeftBoundary" - 101: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelRightBoundary" - 102: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelBottomBoundary" - 103: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelTopBoundary" - 104: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 105: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 106: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 107: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 108: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 109: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 110: 0%....50%....100% - 10 passes
==PROF== Profiling "updateUnknownsKernel" - 111: 0%....50%....100% - 10 passes
                                                                                                                                                                                     Thu Feb  2 16:35:07 2023	[7]: Simulation with max. global dt 0.481155 at time: 3.43995 seconds.
Time left:  31.0800 sec ( 26% done) [====================================>                                                                                                        ] /                                                                                                                                                                                     Thu Feb  2 16:35:07 2023	Writing output file at time: 3.9211 seconds
Time left:  31.0800 sec ( 26% done) [====================================>                                                                                                        ] -==PROF== Profiling "kernelHdBufferEdges" - 112: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 113: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 114: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelLeftBoundary" - 115: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelRightBoundary" - 116: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelBottomBoundary" - 117: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelTopBoundary" - 118: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 119: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 120: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 121: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 122: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 123: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 124: 0%....50%....100% - 10 passes
==PROF== Profiling "updateUnknownsKernel" - 125: 0%....50%....100% - 10 passes
                                                                                                                                                                                     Thu Feb  2 16:35:08 2023	[8]: Simulation with max. global dt 0.481216 at time: 3.9211 seconds.
Time left:  28.8875 sec ( 29% done) [=========================================>                                                                                                   ] \==PROF== Profiling "kernelHdBufferEdges" - 126: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 127: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 128: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelLeftBoundary" - 129: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelRightBoundary" - 130: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelBottomBoundary" - 131: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelTopBoundary" - 132: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 133: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 134: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 135: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 136: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 137: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 138: 0%....50%....100% - 10 passes
==PROF== Profiling "updateUnknownsKernel" - 139: 0%....50%....100% - 10 passes
                                                                                                                                                                                     Thu Feb  2 16:35:09 2023	[9]: Simulation with max. global dt 0.481815 at time: 4.40232 seconds.
Time left:  26.9252 sec ( 32% done) [=============================================>                                                                                               ] |                                                                                                                                                                                     Thu Feb  2 16:35:09 2023	Writing output file at time: 4.88413 seconds
Time left:  26.9252 sec ( 32% done) [=============================================>                                                                                               ] /==PROF== Profiling "kernelHdBufferEdges" - 140: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 141: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 142: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelLeftBoundary" - 143: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelRightBoundary" - 144: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelBottomBoundary" - 145: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelTopBoundary" - 146: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 147: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 148: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 149: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 150: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 151: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 152: 0%....50%....100% - 10 passes
==PROF== Profiling "updateUnknownsKernel" - 153: 0%....50%....100% - 10 passes
                                                                                                                                                                                     Thu Feb  2 16:35:11 2023	[10]: Simulation with max. global dt 0.482879 at time: 4.88413 seconds.
Time left:  26.9228 sec ( 35% done) [==================================================>                                                                                          ] -                                                                                                                                                                                     Thu Feb  2 16:35:11 2023	Writing output file at time: 5.36701 seconds
Time left:  26.9228 sec ( 35% done) [==================================================>                                                                                          ] \==PROF== Profiling "kernelHdBufferEdges" - 154: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 155: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 156: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelLeftBoundary" - 157: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelRightBoundary" - 158: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelBottomBoundary" - 159: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelTopBoundary" - 160: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 161: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 162: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 163: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 164: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 165: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 166: 0%....50%....100% - 10 passes
==PROF== Profiling "updateUnknownsKernel" - 167: 0%....50%....100% - 10 passes
                                                                                                                                                                                     Thu Feb  2 16:35:12 2023	[11]: Simulation with max. global dt 0.484331 at time: 5.36701 seconds.
Time left:  25.0162 sec ( 39% done) [=======================================================>                                                                                     ] |==PROF== Profiling "kernelHdBufferEdges" - 168: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 169: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 170: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelLeftBoundary" - 171: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelRightBoundary" - 172: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelBottomBoundary" - 173: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelTopBoundary" - 174: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 175: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 176: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 177: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 178: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 179: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 180: 0%....50%....100% - 10 passes
==PROF== Profiling "updateUnknownsKernel" - 181: 0%....50%....100% - 10 passes
                                                                                                                                                                                     Thu Feb  2 16:35:13 2023	[12]: Simulation with max. global dt 0.485397 at time: 5.85134 seconds.
Time left:  23.2415 sec ( 42% done) [===========================================================>                                                                                 ] /                                                                                                                                                                                     Thu Feb  2 16:35:13 2023	Writing output file at time: 6.33674 seconds
Time left:  23.2415 sec ( 42% done) [===========================================================>                                                                                 ] -==PROF== Profiling "kernelHdBufferEdges" - 182: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 183: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 184: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelLeftBoundary" - 185: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelRightBoundary" - 186: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelBottomBoundary" - 187: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelTopBoundary" - 188: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 189: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 190: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 191: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 192: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 193: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 194: 0%....50%....100% - 10 passes
==PROF== Profiling "updateUnknownsKernel" - 195: 0%....50%....100% - 10 passes
                                                                                                                                                                                     Thu Feb  2 16:35:14 2023	[13]: Simulation with max. global dt 0.487122 at time: 6.33674 seconds.
Time left:  21.5670 sec ( 45% done) [================================================================>                                                                            ] \                                                                                                                                                                                     Thu Feb  2 16:35:14 2023	Writing output file at time: 6.82386 seconds
Time left:  21.5670 sec ( 45% done) [================================================================>                                                                            ] |==PROF== Profiling "kernelHdBufferEdges" - 196: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 197: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 198: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelLeftBoundary" - 199: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelRightBoundary" - 200: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelBottomBoundary" - 201: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelTopBoundary" - 202: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 203: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 204: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 205: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 206: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 207: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 208: 0%....50%....100% - 10 passes
==PROF== Profiling "updateUnknownsKernel" - 209: 0%....50%....100% - 10 passes
                                                                                                                                                                                     Thu Feb  2 16:35:16 2023	[14]: Simulation with max. global dt 0.489565 at time: 6.82386 seconds.
Time left:  21.0204 sec ( 48% done) [====================================================================>                                                                        ] /==PROF== Profiling "kernelHdBufferEdges" - 210: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 211: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 212: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelLeftBoundary" - 213: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelRightBoundary" - 214: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelBottomBoundary" - 215: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelTopBoundary" - 216: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 217: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 218: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 219: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 220: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 221: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 222: 0%....50%....100% - 10 passes
==PROF== Profiling "updateUnknownsKernel" - 223: 0%....50%....100% - 10 passes
                                                                                                                                                                                     Thu Feb  2 16:35:17 2023	[15]: Simulation with max. global dt 0.492820 at time: 7.31343 seconds.
Time left:  19.3523 sec ( 52% done) [=========================================================================>                                                                   ] -                                                                                                                                                                                     Thu Feb  2 16:35:17 2023	Writing output file at time: 7.80625 seconds
Time left:  19.3523 sec ( 52% done) [=========================================================================>                                                                   ] \==PROF== Profiling "kernelHdBufferEdges" - 224: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 225: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 226: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelLeftBoundary" - 227: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelRightBoundary" - 228: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelBottomBoundary" - 229: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelTopBoundary" - 230: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 231: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 232: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 233: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 234: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 235: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 236: 0%....50%....100% - 10 passes
==PROF== Profiling "updateUnknownsKernel" - 237: 0%....50%....100% - 10 passes
                                                                                                                                                                                     Thu Feb  2 16:35:18 2023	[16]: Simulation with max. global dt 0.494986 at time: 7.80625 seconds.
Time left:  17.7531 sec ( 55% done) [==============================================================================>                                                              ] |                                                                                                                                                                                     Thu Feb  2 16:35:18 2023	Writing output file at time: 8.30123 seconds
Time left:  17.7531 sec ( 55% done) [==============================================================================>                                                              ] /==PROF== Profiling "kernelHdBufferEdges" - 238: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 239: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 240: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelLeftBoundary" - 241: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelRightBoundary" - 242: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelBottomBoundary" - 243: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelTopBoundary" - 244: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 245: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 246: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 247: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 248: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 249: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 250: 0%....50%....100% - 10 passes
==PROF== Profiling "updateUnknownsKernel" - 251: 0%....50%....100% - 10 passes
                                                                                                                                                                                     Thu Feb  2 16:35:20 2023	[17]: Simulation with max. global dt 0.496763 at time: 8.30123 seconds.
Time left:  16.9184 sec ( 58% done) [==================================================================================>                                                          ] -==PROF== Profiling "kernelHdBufferEdges" - 252: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 253: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 254: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelLeftBoundary" - 255: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelRightBoundary" - 256: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelBottomBoundary" - 257: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelTopBoundary" - 258: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 259: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 260: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 261: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 262: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 263: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 264: 0%....50%....100% - 10 passes
==PROF== Profiling "updateUnknownsKernel" - 265: 0%....50%....100% - 10 passes
                                                                                                                                                                                     Thu Feb  2 16:35:21 2023	[18]: Simulation with max. global dt 0.498512 at time: 8.798 seconds.
Time left:  15.3377 sec ( 61% done) [=======================================================================================>                                                     ] \                                                                                                                                                                                     Thu Feb  2 16:35:21 2023	Writing output file at time: 9.29651 seconds
Time left:  15.3377 sec ( 61% done) [=======================================================================================>                                                     ] |==PROF== Profiling "kernelHdBufferEdges" - 266: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 267: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 268: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelLeftBoundary" - 269: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelRightBoundary" - 270: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelBottomBoundary" - 271: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelTopBoundary" - 272: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 273: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 274: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 275: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 276: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 277: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 278: 0%....50%....100% - 10 passes
==PROF== Profiling "updateUnknownsKernel" - 279: 0%....50%....100% - 10 passes
                                                                                                                                                                                     Thu Feb  2 16:35:22 2023	[19]: Simulation with max. global dt 0.500620 at time: 9.29651 seconds.
Time left:  13.8076 sec ( 65% done) [============================================================================================>                                                ] /                                                                                                                                                                                     Thu Feb  2 16:35:22 2023	Writing output file at time: 9.79713 seconds
Time left:  13.8076 sec ( 65% done) [============================================================================================>                                                ] -==PROF== Profiling "kernelHdBufferEdges" - 280: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 281: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 282: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelLeftBoundary" - 283: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelRightBoundary" - 284: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelBottomBoundary" - 285: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelTopBoundary" - 286: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 287: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 288: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 289: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 290: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 291: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 292: 0%....50%....100% - 10 passes
==PROF== Profiling "updateUnknownsKernel" - 293: 0%....50%....100% - 10 passes
                                                                                                                                                                                     Thu Feb  2 16:35:24 2023	[20]: Simulation with max. global dt 0.502214 at time: 9.79713 seconds.
Time left:  12.7793 sec ( 68% done) [================================================================================================>                                            ] \==PROF== Profiling "kernelHdBufferEdges" - 294: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 295: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 296: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelLeftBoundary" - 297: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelRightBoundary" - 298: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelBottomBoundary" - 299: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelTopBoundary" - 300: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 301: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 302: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 303: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 304: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 305: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 306: 0%....50%....100% - 10 passes
==PROF== Profiling "updateUnknownsKernel" - 307: 0%....50%....100% - 10 passes
                                                                                                                                                                                     Thu Feb  2 16:35:25 2023	[21]: Simulation with max. global dt 0.504118 at time: 10.2993 seconds.
Time left:  11.2649 sec ( 72% done) [=====================================================================================================>                                       ] |                                                                                                                                                                                     Thu Feb  2 16:35:25 2023	Writing output file at time: 10.8035 seconds
Time left:  11.2649 sec ( 72% done) [=====================================================================================================>                                       ] /==PROF== Profiling "kernelHdBufferEdges" - 308: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 309: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 310: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelLeftBoundary" - 311: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelRightBoundary" - 312: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelBottomBoundary" - 313: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelTopBoundary" - 314: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 315: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 316: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 317: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 318: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 319: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 320: 0%....50%....100% - 10 passes
==PROF== Profiling "updateUnknownsKernel" - 321: 0%....50%....100% - 10 passes
                                                                                                                                                                                     Thu Feb  2 16:35:26 2023	[22]: Simulation with max. global dt 0.506146 at time: 10.8035 seconds.
Time left:  9.78918 sec ( 75% done) [==========================================================================================================>                                  ] -                                                                                                                                                                                     Thu Feb  2 16:35:26 2023	Writing output file at time: 11.3096 seconds
Time left:  9.78918 sec ( 75% done) [==========================================================================================================>                                  ] \==PROF== Profiling "kernelHdBufferEdges" - 322: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 323: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 324: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelLeftBoundary" - 325: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelRightBoundary" - 326: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelBottomBoundary" - 327: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelTopBoundary" - 328: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 329: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 330: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 331: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 332: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 333: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 334: 0%....50%....100% - 10 passes
==PROF== Profiling "updateUnknownsKernel" - 335: 0%....50%....100% - 10 passes
                                                                                                                                                                                     Thu Feb  2 16:35:28 2023	[23]: Simulation with max. global dt 0.507750 at time: 11.3096 seconds.
Time left:  8.61822 sec ( 78% done) [===============================================================================================================>                             ] |==PROF== Profiling "kernelHdBufferEdges" - 336: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 337: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 338: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelLeftBoundary" - 339: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelRightBoundary" - 340: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelBottomBoundary" - 341: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelTopBoundary" - 342: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 343: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 344: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 345: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 346: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 347: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 348: 0%....50%....100% - 10 passes
==PROF== Profiling "updateUnknownsKernel" - 349: 0%....50%....100% - 10 passes
                                                                                                                                                                                     Thu Feb  2 16:35:29 2023	[24]: Simulation with max. global dt 0.509848 at time: 11.8174 seconds.
Time left:  7.15509 sec ( 82% done) [===================================================================================================================>                         ] /                                                                                                                                                                                     Thu Feb  2 16:35:29 2023	Writing output file at time: 12.3272 seconds
Time left:  7.15509 sec ( 82% done) [===================================================================================================================>                         ] -==PROF== Profiling "kernelHdBufferEdges" - 350: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 351: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 352: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelLeftBoundary" - 353: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelRightBoundary" - 354: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelBottomBoundary" - 355: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelTopBoundary" - 356: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 357: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 358: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 359: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 360: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 361: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 362: 0%....50%....100% - 10 passes
==PROF== Profiling "updateUnknownsKernel" - 363: 0%....50%....100% - 10 passes
                                                                                                                                                                                     Thu Feb  2 16:35:30 2023	[25]: Simulation with max. global dt 0.511555 at time: 12.3272 seconds.
Time left:  5.72346 sec ( 85% done) [========================================================================================================================>                    ] \                                                                                                                                                                                     Thu Feb  2 16:35:30 2023	Writing output file at time: 12.8388 seconds
Time left:  5.72346 sec ( 85% done) [========================================================================================================================>                    ] |==PROF== Profiling "kernelHdBufferEdges" - 364: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 365: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 366: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelLeftBoundary" - 367: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelRightBoundary" - 368: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelBottomBoundary" - 369: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelTopBoundary" - 370: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 371: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 372: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 373: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 374: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 375: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 376: 0%....50%....100% - 10 passes
==PROF== Profiling "updateUnknownsKernel" - 377: 0%....50%....100% - 10 passes
                                                                                                                                                                                     Thu Feb  2 16:35:31 2023	[26]: Simulation with max. global dt 0.513350 at time: 12.8388 seconds.
Time left:  4.31963 sec ( 89% done) [=============================================================================================================================>               ] /==PROF== Profiling "kernelHdBufferEdges" - 378: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 379: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 380: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelLeftBoundary" - 381: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelRightBoundary" - 382: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelBottomBoundary" - 383: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelTopBoundary" - 384: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 385: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 386: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 387: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 388: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 389: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 390: 0%....50%....100% - 10 passes
==PROF== Profiling "updateUnknownsKernel" - 391: 0%....50%....100% - 10 passes
                                                                                                                                                                                     Thu Feb  2 16:35:33 2023	[27]: Simulation with max. global dt 0.515553 at time: 13.3521 seconds.
Time left:  3.02116 sec ( 92% done) [==================================================================================================================================>          ] -                                                                                                                                                                                     Thu Feb  2 16:35:33 2023	Writing output file at time: 13.8677 seconds
Time left:  3.02116 sec ( 92% done) [==================================================================================================================================>          ] \==PROF== Profiling "kernelHdBufferEdges" - 392: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 393: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 394: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelLeftBoundary" - 395: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelRightBoundary" - 396: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelBottomBoundary" - 397: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelTopBoundary" - 398: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 399: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 400: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 401: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 402: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 403: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 404: 0%....50%....100% - 10 passes
==PROF== Profiling "updateUnknownsKernel" - 405: 0%....50%....100% - 10 passes
                                                                                                                                                                                     Thu Feb  2 16:35:34 2023	[28]: Simulation with max. global dt 0.516978 at time: 13.8677 seconds.
Time left:  1.62560 sec ( 95% done) [=======================================================================================================================================>     ] |                                                                                                                                                                                     Thu Feb  2 16:35:34 2023	Writing output file at time: 14.3846 seconds
Time left:  1.62560 sec ( 95% done) [=======================================================================================================================================>     ] /==PROF== Profiling "kernelHdBufferEdges" - 406: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 407: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 408: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelLeftBoundary" - 409: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelRightBoundary" - 410: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelBottomBoundary" - 411: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelTopBoundary" - 412: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 413: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 414: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 415: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 416: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 417: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 418: 0%....50%....100% - 10 passes
==PROF== Profiling "updateUnknownsKernel" - 419: 0%....50%....100% - 10 passes
                                                                                                                                                                                     Thu Feb  2 16:35:35 2023	[29]: Simulation with max. global dt 0.518982 at time: 14.3846 seconds.
Time left:      < 1 sec ( 99% done) [============================================================================================================================================>] -==PROF== Profiling "kernelHdBufferEdges" - 420: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 421: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelHdBufferEdges" - 422: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelLeftBoundary" - 423: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelRightBoundary" - 424: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelBottomBoundary" - 425: 0%....50%....100% - 10 passes
==PROF== Profiling "kernelTopBoundary" - 426: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 427: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 428: 0%....50%....100% - 10 passes
==PROF== Profiling "computeNetUpdatesKernel" - 429: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 430: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 431: 0%....50%....100% - 10 passes
==PROF== Profiling "_kernel_agent" - 432: 0%....50%....100% - 10 passes
==PROF== Profiling "updateUnknownsKernel" - 433: 0%....50%....100% - 10 passes
                                                                                                                                                                                     Thu Feb  2 16:35:37 2023	[30]: Simulation with max. global dt 0.520747 at time: 14.9036 seconds.
Time left:      < 1 sec (102% done) [================================================================================================================================================] \                                                                                                                                                                                     Thu Feb  2 16:35:37 2023	Writing output file at time: 15.4244 seconds
Time left:      < 1 sec (102% done) [================================================================================================================================================] |[0m[32m
[INF]	Total runtime for [whole_simulation_loop] 40.40618167130 sec[0m
------------------------------------------------------------------
Thu Feb  2 16:35:37 2023	Simulation finished. Printing statistics for each process.
------------------------------------------------------------------
Thu Feb  2 16:35:37 2023	Process 0 - CPU Time: 38.0092 seconds
Thu Feb  2 16:35:37 2023	Process 0 - CPU + Communication Time: 38.0093 seconds
Thu Feb  2 16:35:37 2023	Process 0 - Wall clock time: 41.00000 seconds
Thu Feb  2 16:35:37 2023	31 Iterations done

*************************************************************
SWE Finished successfully.
*************************************************************
Thu Feb  2 16:35:37 2023	Process 0 - Resetting the CUDA devices==PROF== Disconnected from process 2375701
[2375701] SWE-MPI-Runner@127.0.0.1
  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.25
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         4017
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.10
    L1/TEX Cache Throughput             %        27.59
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        32.62
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.10
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.13
    SM Frequency            cycle/nsecond         1.27
    Elapsed Cycles                  cycle         3979
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.14
    L1/TEX Cache Throughput             %        28.16
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.96
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.45
    SM Frequency            cycle/nsecond         1.32
    Elapsed Cycles                  cycle         4084
    Memory Throughput                   %         0.45
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.10
    L1/TEX Cache Throughput             %        28.73
    L2 Cache Throughput                 %         0.45
    SM Active Cycles                cycle        31.32
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.10
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelLeftBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.67
    SM Frequency            cycle/nsecond         1.35
    Elapsed Cycles                  cycle         4331
    Memory Throughput                   %         0.55
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.20
    L1/TEX Cache Throughput             %         3.29
    L2 Cache Throughput                 %         0.55
    SM Active Cycles                cycle       273.38
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.06
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelRightBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.14
    SM Frequency            cycle/nsecond         1.27
    Elapsed Cycles                  cycle         4265
    Memory Throughput                   %         0.55
    DRAM Throughput                     %         0.09
    Duration                      usecond         3.36
    L1/TEX Cache Throughput             %         3.37
    L2 Cache Throughput                 %         0.55
    SM Active Cycles                cycle       267.03
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.12
    Achieved Active Warps Per SM           warp         1.02
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelBottomBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.44
    SM Frequency            cycle/nsecond         1.31
    Elapsed Cycles                  cycle         4402
    Memory Throughput                   %         0.64
    DRAM Throughput                     %         0.28
    Duration                      usecond         3.36
    L1/TEX Cache Throughput             %         3.32
    L2 Cache Throughput                 %         0.64
    SM Active Cycles                cycle          279
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelTopBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.72
    SM Frequency            cycle/nsecond         1.35
    Elapsed Cycles                  cycle         4423
    Memory Throughput                   %         0.64
    DRAM Throughput                     %         0.28
    Duration                      usecond         3.26
    L1/TEX Cache Throughput             %         3.36
    L2 Cache Throughput                 %         0.64
    SM Active Cycles                cycle       275.47
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.96
    SM Frequency            cycle/nsecond         1.39
    Elapsed Cycles                  cycle        20163
    Memory Throughput                   %         2.61
    DRAM Throughput                     %         1.32
    Duration                      usecond        14.46
    L1/TEX Cache Throughput             %         2.65
    L2 Cache Throughput                 %         2.61
    SM Active Cycles                cycle     16786.09
    Compute (SM) Throughput             %        43.64
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           18
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (66.7%) and measured achieved occupancy (4.2%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (1, 8, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.55
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle        10917
    Memory Throughput                   %         0.92
    DRAM Throughput                     %         0.05
    Duration                      usecond         8.19
    L1/TEX Cache Throughput             %         0.91
    L2 Cache Throughput                 %         0.92
    SM Active Cycles                cycle      1056.18
    Compute (SM) Throughput             %         2.50
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 1, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.60
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle        11136
    Memory Throughput                   %         0.94
    DRAM Throughput                     %         0.11
    Duration                      usecond         8.32
    L1/TEX Cache Throughput             %         1.06
    L2 Cache Throughput                 %         0.94
    SM Active Cycles                cycle      1073.63
    Compute (SM) Throughput             %         2.48
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__reduce::ReduceAgent<thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>, thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>(T2, T3, T4, T5) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.77
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle         5077
    Memory Throughput                   %         2.82
    DRAM Throughput                     %         0.03
    Duration                      usecond         4.19
    L1/TEX Cache Throughput             %        18.82
    L2 Cache Throughput                 %         2.82
    SM Active Cycles                cycle        47.90
    Compute (SM) Throughput             %         0.15
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.06
    Achieved Active Warps Per SM           warp         6.75
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (14.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>, thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.06
    SM Frequency            cycle/nsecond         1.25
    Elapsed Cycles                  cycle         3162
    Memory Throughput                   %         0.66
    DRAM Throughput                     %         0.01
    Duration                      usecond         2.53
    L1/TEX Cache Throughput             %        45.03
    L2 Cache Throughput                 %         0.66
    SM Active Cycles                cycle        19.99
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        11.95
    Achieved Active Warps Per SM           warp         5.74
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>, thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.48
    SM Frequency            cycle/nsecond         1.47
    Elapsed Cycles                  cycle         2589
    Memory Throughput                   %         0.61
    DRAM Throughput                     %            0
    Duration                      usecond         1.76
    L1/TEX Cache Throughput             %       208.16
    L2 Cache Throughput                 %         0.61
    SM Active Cycles                cycle         4.32
    Compute (SM) Throughput             %         0.00
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.18
    Achieved Active Warps Per SM           warp         7.77
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (16.2%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::updateUnknownsKernel(const double *, const double *, const double *, const double *, const double *, const double *, const double *, const double *, double *, double *, double *, double, double, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.79
    SM Frequency            cycle/nsecond         1.37
    Elapsed Cycles                  cycle         5746
    Memory Throughput                   %        12.48
    DRAM Throughput                     %        12.48
    Duration                      usecond         4.19
    L1/TEX Cache Throughput             %         7.74
    L2 Cache Throughput                 %        10.22
    SM Active Cycles                cycle      3014.47
    Compute (SM) Throughput             %         4.72
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (66.7%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (66.7%) and measured achieved occupancy (4.2%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.48
    SM Frequency            cycle/nsecond         1.32
    Elapsed Cycles                  cycle         4102
    Memory Throughput                   %         0.45
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.10
    L1/TEX Cache Throughput             %        27.65
    L2 Cache Throughput                 %         0.45
    SM Active Cycles                cycle        32.54
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.11
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.25
    SM Frequency            cycle/nsecond         1.28
    Elapsed Cycles                  cycle         3981
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.10
    L1/TEX Cache Throughput             %        27.98
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        32.16
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.33
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         3961
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.02
    Duration                      usecond         3.07
    L1/TEX Cache Throughput             %        28.67
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.40
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelLeftBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.63
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle         4366
    Memory Throughput                   %         0.54
    DRAM Throughput                     %         0.09
    Duration                      usecond         3.26
    L1/TEX Cache Throughput             %         3.19
    L2 Cache Throughput                 %         0.54
    SM Active Cycles                cycle       281.76
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.06
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelRightBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.44
    SM Frequency            cycle/nsecond         1.31
    Elapsed Cycles                  cycle         4403
    Memory Throughput                   %         0.53
    DRAM Throughput                     %         0.07
    Duration                      usecond         3.36
    L1/TEX Cache Throughput             %         3.28
    L2 Cache Throughput                 %         0.53
    SM Active Cycles                cycle       274.29
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelBottomBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.24
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         4361
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.28
    Duration                      usecond         3.39
    L1/TEX Cache Throughput             %         3.35
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle       276.40
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.11
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelTopBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.53
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle         4481
    Memory Throughput                   %         0.63
    DRAM Throughput                     %         0.27
    Duration                      usecond         3.36
    L1/TEX Cache Throughput             %         3.23
    L2 Cache Throughput                 %         0.63
    SM Active Cycles                cycle       286.60
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.10
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.98
    SM Frequency            cycle/nsecond         1.40
    Elapsed Cycles                  cycle        20148
    Memory Throughput                   %         2.62
    DRAM Throughput                     %         1.32
    Duration                      usecond        14.40
    L1/TEX Cache Throughput             %         2.65
    L2 Cache Throughput                 %         2.62
    SM Active Cycles                cycle     16845.74
    Compute (SM) Throughput             %        43.67
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           18
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.14
    Achieved Active Warps Per SM           warp         1.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (66.7%) and measured achieved occupancy (4.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (1, 8, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.54
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle        10932
    Memory Throughput                   %         0.92
    DRAM Throughput                     %         0.06
    Duration                      usecond         8.22
    L1/TEX Cache Throughput             %         0.91
    L2 Cache Throughput                 %         0.92
    SM Active Cycles                cycle      1053.79
    Compute (SM) Throughput             %         2.50
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 1, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.60
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle        11117
    Memory Throughput                   %         0.94
    DRAM Throughput                     %         0.11
    Duration                      usecond         8.29
    L1/TEX Cache Throughput             %         1.06
    L2 Cache Throughput                 %         0.94
    SM Active Cycles                cycle      1073.09
    Compute (SM) Throughput             %         2.49
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__reduce::ReduceAgent<thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>, thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>(T2, T3, T4, T5) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.72
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle         5055
    Memory Throughput                   %         2.83
    DRAM Throughput                     %         0.03
    Duration                      usecond         4.19
    L1/TEX Cache Throughput             %        18.87
    L2 Cache Throughput                 %         2.83
    SM Active Cycles                cycle        47.76
    Compute (SM) Throughput             %         0.15
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.17
    Achieved Active Warps Per SM           warp         6.80
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (14.2%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>, thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.21
    SM Frequency            cycle/nsecond         1.28
    Elapsed Cycles                  cycle         3186
    Memory Throughput                   %         0.66
    DRAM Throughput                     %         0.01
    Duration                      usecond         2.50
    L1/TEX Cache Throughput             %        44.25
    L2 Cache Throughput                 %         0.66
    SM Active Cycles                cycle        20.34
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        11.93
    Achieved Active Warps Per SM           warp         5.73
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (11.9%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>, thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.84
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle         2595
    Memory Throughput                   %         0.60
    DRAM Throughput                     %            0
    Duration                      usecond         1.70
    L1/TEX Cache Throughput             %       222.55
    L2 Cache Throughput                 %         0.60
    SM Active Cycles                cycle         4.04
    Compute (SM) Throughput             %         0.00
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.08
    Achieved Active Warps Per SM           warp         7.72
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (16.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::updateUnknownsKernel(const double *, const double *, const double *, const double *, const double *, const double *, const double *, const double *, double *, double *, double *, double, double, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.33
    SM Frequency            cycle/nsecond         1.46
    Elapsed Cycles                  cycle         6012
    Memory Throughput                   %        11.95
    DRAM Throughput                     %        11.95
    Duration                      usecond         4.13
    L1/TEX Cache Throughput             %         7.16
    L2 Cache Throughput                 %         9.79
    SM Active Cycles                cycle      3261.85
    Compute (SM) Throughput             %         4.51
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         3.84
    Achieved Active Warps Per SM           warp         1.84
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (66.7%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (66.7%) and measured achieved occupancy (3.8%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.41
    SM Frequency            cycle/nsecond         1.31
    Elapsed Cycles                  cycle         4075
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.10
    L1/TEX Cache Throughput             %        27.04
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        33.28
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.06
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.25
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         4000
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.02
    Duration                      usecond         3.10
    L1/TEX Cache Throughput             %        28.19
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.93
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.29
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         3933
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.02
    Duration                      usecond         3.04
    L1/TEX Cache Throughput             %        28.87
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.18
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelLeftBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.31
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         4213
    Memory Throughput                   %         0.56
    DRAM Throughput                     %         0.09
    Duration                      usecond         3.26
    L1/TEX Cache Throughput             %         3.31
    L2 Cache Throughput                 %         0.56
    SM Active Cycles                cycle       271.97
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.05
    Achieved Active Warps Per SM           warp         0.98
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.0%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelRightBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.36
    SM Frequency            cycle/nsecond         1.30
    Elapsed Cycles                  cycle         4272
    Memory Throughput                   %         0.55
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.30
    L1/TEX Cache Throughput             %         3.28
    L2 Cache Throughput                 %         0.55
    SM Active Cycles                cycle       274.06
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelBottomBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.10
    SM Frequency            cycle/nsecond         1.26
    Elapsed Cycles                  cycle         4328
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.29
    Duration                      usecond         3.42
    L1/TEX Cache Throughput             %         3.31
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle       279.15
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelTopBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.35
    SM Frequency            cycle/nsecond         1.30
    Elapsed Cycles                  cycle         4362
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.28
    Duration                      usecond         3.36
    L1/TEX Cache Throughput             %         3.33
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle       277.90
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.01
    SM Frequency            cycle/nsecond         1.40
    Elapsed Cycles                  cycle        20132
    Memory Throughput                   %         2.62
    DRAM Throughput                     %         1.32
    Duration                      usecond        14.34
    L1/TEX Cache Throughput             %         2.65
    L2 Cache Throughput                 %         2.62
    SM Active Cycles                cycle     16814.03
    Compute (SM) Throughput             %        43.70
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           18
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.20
    Achieved Active Warps Per SM           warp         2.02
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (66.7%) and measured achieved occupancy (4.2%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (1, 8, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.64
    SM Frequency            cycle/nsecond         1.35
    Elapsed Cycles                  cycle        10957
    Memory Throughput                   %         0.91
    DRAM Throughput                     %         0.06
    Duration                      usecond         8.13
    L1/TEX Cache Throughput             %         0.91
    L2 Cache Throughput                 %         0.91
    SM Active Cycles                cycle      1058.24
    Compute (SM) Throughput             %         2.49
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 1, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.60
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle        11132
    Memory Throughput                   %         0.94
    DRAM Throughput                     %         0.11
    Duration                      usecond         8.32
    L1/TEX Cache Throughput             %         1.06
    L2 Cache Throughput                 %         0.94
    SM Active Cycles                cycle      1075.24
    Compute (SM) Throughput             %         2.49
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__reduce::ReduceAgent<thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>, thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>(T2, T3, T4, T5) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.68
    SM Frequency            cycle/nsecond         1.20
    Elapsed Cycles                  cycle         4992
    Memory Throughput                   %         2.87
    DRAM Throughput                     %         0.03
    Duration                      usecond         4.16
    L1/TEX Cache Throughput             %        19.20
    L2 Cache Throughput                 %         2.87
    SM Active Cycles                cycle        46.96
    Compute (SM) Throughput             %         0.15
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.27
    Achieved Active Warps Per SM           warp         6.85
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (14.3%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>, thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.42
    SM Frequency            cycle/nsecond         1.31
    Elapsed Cycles                  cycle         3179
    Memory Throughput                   %         0.66
    DRAM Throughput                     %         0.01
    Duration                      usecond         2.43
    L1/TEX Cache Throughput             %        45.10
    L2 Cache Throughput                 %         0.66
    SM Active Cycles                cycle        19.96
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        11.95
    Achieved Active Warps Per SM           warp         5.74
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>, thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.29
    SM Frequency            cycle/nsecond         1.60
    Elapsed Cycles                  cycle         2609
    Memory Throughput                   %         0.60
    DRAM Throughput                     %            0
    Duration                      usecond         1.63
    L1/TEX Cache Throughput             %       208.16
    L2 Cache Throughput                 %         0.60
    SM Active Cycles                cycle         4.32
    Compute (SM) Throughput             %         0.00
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.12
    Achieved Active Warps Per SM           warp         7.74
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (16.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::updateUnknownsKernel(const double *, const double *, const double *, const double *, const double *, const double *, const double *, const double *, double *, double *, double *, double, double, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.57
    SM Frequency            cycle/nsecond         1.49
    Elapsed Cycles                  cycle         6060
    Memory Throughput                   %        11.94
    DRAM Throughput                     %        11.94
    Duration                      usecond         4.06
    L1/TEX Cache Throughput             %         7.70
    L2 Cache Throughput                 %         9.75
    SM Active Cycles                cycle      3031.13
    Compute (SM) Throughput             %         4.48
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.19
    Achieved Active Warps Per SM           warp         2.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (66.7%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (66.7%) and measured achieved occupancy (4.2%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.59
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle         4065
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.04
    L1/TEX Cache Throughput             %        27.49
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        32.74
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.16
    SM Frequency            cycle/nsecond         1.27
    Elapsed Cycles                  cycle         3984
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.14
    L1/TEX Cache Throughput             %        28.00
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        32.15
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.50
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle         3970
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.02
    Duration                      usecond         2.98
    L1/TEX Cache Throughput             %        28.57
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.50
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelLeftBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.41
    SM Frequency            cycle/nsecond         1.30
    Elapsed Cycles                  cycle         4260
    Memory Throughput                   %         0.56
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.26
    L1/TEX Cache Throughput             %         3.37
    L2 Cache Throughput                 %         0.56
    SM Active Cycles                cycle       267.37
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelRightBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.48
    SM Frequency            cycle/nsecond         1.32
    Elapsed Cycles                  cycle         4350
    Memory Throughput                   %         0.54
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.30
    L1/TEX Cache Throughput             %         3.28
    L2 Cache Throughput                 %         0.54
    SM Active Cycles                cycle       274.09
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelBottomBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.38
    SM Frequency            cycle/nsecond         1.30
    Elapsed Cycles                  cycle         4365
    Memory Throughput                   %         0.64
    DRAM Throughput                     %         0.28
    Duration                      usecond         3.36
    L1/TEX Cache Throughput             %         3.09
    L2 Cache Throughput                 %         0.64
    SM Active Cycles                cycle       299.81
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         1.94
    Achieved Active Warps Per SM           warp         0.93
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (1.9%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelTopBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.48
    SM Frequency            cycle/nsecond         1.32
    Elapsed Cycles                  cycle         4357
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.27
    Duration                      usecond         3.30
    L1/TEX Cache Throughput             %         3.25
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle       284.78
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.04
    Achieved Active Warps Per SM           warp         0.98
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.0%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.92
    SM Frequency            cycle/nsecond         1.39
    Elapsed Cycles                  cycle        20017
    Memory Throughput                   %         2.64
    DRAM Throughput                     %         1.33
    Duration                      usecond        14.40
    L1/TEX Cache Throughput             %         2.67
    L2 Cache Throughput                 %         2.64
    SM Active Cycles                cycle     16808.40
    Compute (SM) Throughput             %        43.95
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           18
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (66.7%) and measured achieved occupancy (4.2%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (1, 8, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.61
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle        10931
    Memory Throughput                   %         0.92
    DRAM Throughput                     %         0.06
    Duration                      usecond         8.16
    L1/TEX Cache Throughput             %         0.91
    L2 Cache Throughput                 %         0.92
    SM Active Cycles                cycle      1057.04
    Compute (SM) Throughput             %         2.50
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 1, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.68
    SM Frequency            cycle/nsecond         1.35
    Elapsed Cycles                  cycle        11103
    Memory Throughput                   %         0.94
    DRAM Throughput                     %         0.11
    Duration                      usecond         8.22
    L1/TEX Cache Throughput             %         1.06
    L2 Cache Throughput                 %         0.94
    SM Active Cycles                cycle      1073.32
    Compute (SM) Throughput             %         2.49
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__reduce::ReduceAgent<thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>, thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>(T2, T3, T4, T5) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.83
    SM Frequency            cycle/nsecond         1.22
    Elapsed Cycles                  cycle         5072
    Memory Throughput                   %         2.82
    DRAM Throughput                     %         0.03
    Duration                      usecond         4.16
    L1/TEX Cache Throughput             %        18.86
    L2 Cache Throughput                 %         2.82
    SM Active Cycles                cycle        47.81
    Compute (SM) Throughput             %         0.15
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.08
    Achieved Active Warps Per SM           warp         6.76
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (14.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>, thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.98
    SM Frequency            cycle/nsecond         1.25
    Elapsed Cycles                  cycle         3160
    Memory Throughput                   %         0.66
    DRAM Throughput                     %         0.01
    Duration                      usecond         2.53
    L1/TEX Cache Throughput             %        44.64
    L2 Cache Throughput                 %         0.66
    SM Active Cycles                cycle        20.16
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        12.01
    Achieved Active Warps Per SM           warp         5.76
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>, thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.58
    SM Frequency            cycle/nsecond         1.64
    Elapsed Cycles                  cycle         2579
    Memory Throughput                   %         0.61
    DRAM Throughput                     %            0
    Duration                      usecond         1.57
    L1/TEX Cache Throughput             %       222.55
    L2 Cache Throughput                 %         0.61
    SM Active Cycles                cycle         4.04
    Compute (SM) Throughput             %         0.00
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.08
    Achieved Active Warps Per SM           warp         7.72
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (16.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::updateUnknownsKernel(const double *, const double *, const double *, const double *, const double *, const double *, const double *, const double *, double *, double *, double *, double, double, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.60
    SM Frequency            cycle/nsecond         1.49
    Elapsed Cycles                  cycle         6022
    Memory Throughput                   %        11.89
    DRAM Throughput                     %        11.89
    Duration                      usecond         4.03
    L1/TEX Cache Throughput             %         7.75
    L2 Cache Throughput                 %         9.77
    SM Active Cycles                cycle      3013.47
    Compute (SM) Throughput             %         4.50
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.12
    Achieved Active Warps Per SM           warp         1.98
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (66.7%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (66.7%) and measured achieved occupancy (4.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.42
    SM Frequency            cycle/nsecond         1.32
    Elapsed Cycles                  cycle         4008
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.04
    L1/TEX Cache Throughput             %        27.91
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        32.25
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.12
    Achieved Active Warps Per SM           warp         1.02
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.58
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle         4011
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.01
    L1/TEX Cache Throughput             %        28.02
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        32.12
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.34
    SM Frequency            cycle/nsecond         1.31
    Elapsed Cycles                  cycle         3932
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.02
    Duration                      usecond         3.01
    L1/TEX Cache Throughput             %        28.43
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.66
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.21
    Achieved Active Warps Per SM           warp         1.06
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.2%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelLeftBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.43
    SM Frequency            cycle/nsecond         1.31
    Elapsed Cycles                  cycle         4233
    Memory Throughput                   %         0.56
    DRAM Throughput                     %         0.27
    Duration                      usecond         3.23
    L1/TEX Cache Throughput             %         3.40
    L2 Cache Throughput                 %         0.56
    SM Active Cycles                cycle       264.47
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.13
    Achieved Active Warps Per SM           warp         1.02
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelRightBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.28
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         4213
    Memory Throughput                   %         0.56
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.26
    L1/TEX Cache Throughput             %         3.30
    L2 Cache Throughput                 %         0.56
    SM Active Cycles                cycle       272.78
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelBottomBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.54
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle         4385
    Memory Throughput                   %         0.64
    DRAM Throughput                     %         0.27
    Duration                      usecond         3.30
    L1/TEX Cache Throughput             %         3.30
    L2 Cache Throughput                 %         0.64
    SM Active Cycles                cycle       280.18
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelTopBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.76
    SM Frequency            cycle/nsecond         1.37
    Elapsed Cycles                  cycle         4517
    Memory Throughput                   %         0.62
    DRAM Throughput                     %         0.28
    Duration                      usecond         3.30
    L1/TEX Cache Throughput             %         3.33
    L2 Cache Throughput                 %         0.62
    SM Active Cycles                cycle       277.99
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.11
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.98
    SM Frequency            cycle/nsecond         1.40
    Elapsed Cycles                  cycle        20141
    Memory Throughput                   %         2.62
    DRAM Throughput                     %         1.32
    Duration                      usecond        14.40
    L1/TEX Cache Throughput             %         2.65
    L2 Cache Throughput                 %         2.62
    SM Active Cycles                cycle     16830.26
    Compute (SM) Throughput             %        43.68
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           18
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.18
    Achieved Active Warps Per SM           warp         2.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (66.7%) and measured achieved occupancy (4.2%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (1, 8, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.56
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle        10946
    Memory Throughput                   %         0.91
    DRAM Throughput                     %         0.06
    Duration                      usecond         8.19
    L1/TEX Cache Throughput             %         0.91
    L2 Cache Throughput                 %         0.91
    SM Active Cycles                cycle      1057.32
    Compute (SM) Throughput             %         2.49
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 1, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.65
    SM Frequency            cycle/nsecond         1.35
    Elapsed Cycles                  cycle        11176
    Memory Throughput                   %         0.94
    DRAM Throughput                     %         0.11
    Duration                      usecond         8.29
    L1/TEX Cache Throughput             %         1.06
    L2 Cache Throughput                 %         0.94
    SM Active Cycles                cycle      1073.78
    Compute (SM) Throughput             %         2.48
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__reduce::ReduceAgent<thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>, thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>(T2, T3, T4, T5) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.11
    SM Frequency            cycle/nsecond         1.42
    Elapsed Cycles                  cycle         5899
    Memory Throughput                   %         2.45
    DRAM Throughput                     %         0.03
    Duration                      usecond         4.16
    L1/TEX Cache Throughput             %        19.02
    L2 Cache Throughput                 %         2.45
    SM Active Cycles                cycle        47.40
    Compute (SM) Throughput             %         0.13
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.23
    Achieved Active Warps Per SM           warp         6.83
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (14.2%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>, thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond            8
    SM Frequency            cycle/nsecond         1.24
    Elapsed Cycles                  cycle         3184
    Memory Throughput                   %         0.66
    DRAM Throughput                     %         0.02
    Duration                      usecond         2.56
    L1/TEX Cache Throughput             %        44.70
    L2 Cache Throughput                 %         0.66
    SM Active Cycles                cycle        20.13
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        11.85
    Achieved Active Warps Per SM           warp         5.69
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (11.9%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>, thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.48
    SM Frequency            cycle/nsecond         1.47
    Elapsed Cycles                  cycle         2588
    Memory Throughput                   %         0.61
    DRAM Throughput                     %            0
    Duration                      usecond         1.76
    L1/TEX Cache Throughput             %       207.46
    L2 Cache Throughput                 %         0.61
    SM Active Cycles                cycle         4.34
    Compute (SM) Throughput             %         0.00
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.07
    Achieved Active Warps Per SM           warp         7.71
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (16.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::updateUnknownsKernel(const double *, const double *, const double *, const double *, const double *, const double *, const double *, const double *, double *, double *, double *, double, double, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.43
    SM Frequency            cycle/nsecond         1.47
    Elapsed Cycles                  cycle         6063
    Memory Throughput                   %        11.82
    DRAM Throughput                     %        11.82
    Duration                      usecond         4.13
    L1/TEX Cache Throughput             %         7.46
    L2 Cache Throughput                 %         9.68
    SM Active Cycles                cycle      3129.76
    Compute (SM) Throughput             %         4.47
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.11
    Achieved Active Warps Per SM           warp         1.97
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (66.7%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (66.7%) and measured achieved occupancy (4.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.27
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         4075
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.17
    L1/TEX Cache Throughput             %        27.78
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        32.40
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.20
    SM Frequency            cycle/nsecond         1.27
    Elapsed Cycles                  cycle         3992
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.02
    Duration                      usecond         3.14
    L1/TEX Cache Throughput             %        28.06
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        32.07
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.17
    SM Frequency            cycle/nsecond         1.28
    Elapsed Cycles                  cycle         3937
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.02
    Duration                      usecond         3.07
    L1/TEX Cache Throughput             %        28.64
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.43
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelLeftBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.26
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         4255
    Memory Throughput                   %         0.56
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.30
    L1/TEX Cache Throughput             %         3.29
    L2 Cache Throughput                 %         0.56
    SM Active Cycles                cycle       273.24
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.05
    Achieved Active Warps Per SM           warp         0.98
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.0%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelRightBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.43
    SM Frequency            cycle/nsecond         1.31
    Elapsed Cycles                  cycle         4364
    Memory Throughput                   %         0.54
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.33
    L1/TEX Cache Throughput             %         3.23
    L2 Cache Throughput                 %         0.54
    SM Active Cycles                cycle       278.38
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelBottomBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.16
    SM Frequency            cycle/nsecond         1.27
    Elapsed Cycles                  cycle         4346
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.29
    Duration                      usecond         3.42
    L1/TEX Cache Throughput             %         3.24
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle       285.84
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.04
    Achieved Active Warps Per SM           warp         0.98
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.0%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelTopBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.32
    SM Frequency            cycle/nsecond         1.30
    Elapsed Cycles                  cycle         4354
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.28
    Duration                      usecond         3.36
    L1/TEX Cache Throughput             %         3.30
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle       280.01
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.10
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.00
    SM Frequency            cycle/nsecond         1.40
    Elapsed Cycles                  cycle        20062
    Memory Throughput                   %         2.63
    DRAM Throughput                     %         1.33
    Duration                      usecond        14.30
    L1/TEX Cache Throughput             %         2.66
    L2 Cache Throughput                 %         2.63
    SM Active Cycles                cycle     16773.01
    Compute (SM) Throughput             %        43.65
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           18
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (66.7%) and measured achieved occupancy (4.2%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (1, 8, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.56
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle        10913
    Memory Throughput                   %         0.92
    DRAM Throughput                     %         0.05
    Duration                      usecond         8.19
    L1/TEX Cache Throughput             %         0.91
    L2 Cache Throughput                 %         0.92
    SM Active Cycles                cycle      1056.93
    Compute (SM) Throughput             %         2.50
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 1, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.59
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle        11090
    Memory Throughput                   %         0.94
    DRAM Throughput                     %         0.11
    Duration                      usecond         8.29
    L1/TEX Cache Throughput             %         1.06
    L2 Cache Throughput                 %         0.94
    SM Active Cycles                cycle      1071.51
    Compute (SM) Throughput             %         2.50
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__reduce::ReduceAgent<thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>, thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>(T2, T3, T4, T5) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.06
    SM Frequency            cycle/nsecond         1.41
    Elapsed Cycles                  cycle         5866
    Memory Throughput                   %         2.44
    DRAM Throughput                     %         0.03
    Duration                      usecond         4.16
    L1/TEX Cache Throughput             %        18.87
    L2 Cache Throughput                 %         2.44
    SM Active Cycles                cycle        47.78
    Compute (SM) Throughput             %         0.13
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        13.91
    Achieved Active Warps Per SM           warp         6.68
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (13.9%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>, thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.23
    SM Frequency            cycle/nsecond         1.28
    Elapsed Cycles                  cycle         3162
    Memory Throughput                   %         0.66
    DRAM Throughput                     %         0.01
    Duration                      usecond         2.46
    L1/TEX Cache Throughput             %        44.74
    L2 Cache Throughput                 %         0.66
    SM Active Cycles                cycle        20.12
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        12.06
    Achieved Active Warps Per SM           warp         5.79
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>, thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.03
    SM Frequency            cycle/nsecond         1.56
    Elapsed Cycles                  cycle         2601
    Memory Throughput                   %         0.60
    DRAM Throughput                     %            0
    Duration                      usecond         1.66
    L1/TEX Cache Throughput             %       222.55
    L2 Cache Throughput                 %         0.60
    SM Active Cycles                cycle         4.04
    Compute (SM) Throughput             %         0.00
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.14
    Achieved Active Warps Per SM           warp         7.75
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (16.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::updateUnknownsKernel(const double *, const double *, const double *, const double *, const double *, const double *, const double *, const double *, double *, double *, double *, double, double, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.31
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         5344
    Memory Throughput                   %        13.41
    DRAM Throughput                     %        13.41
    Duration                      usecond         4.13
    L1/TEX Cache Throughput             %         7.74
    L2 Cache Throughput                 %        11.00
    SM Active Cycles                cycle      3015.50
    Compute (SM) Throughput             %         5.08
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.47
    Achieved Active Warps Per SM           warp         2.15
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (66.7%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (66.7%) and measured achieved occupancy (4.5%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.18
    SM Frequency            cycle/nsecond         1.27
    Elapsed Cycles                  cycle         4030
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.02
    Duration                      usecond         3.17
    L1/TEX Cache Throughput             %        27.38
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        32.87
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.35
    SM Frequency            cycle/nsecond         1.30
    Elapsed Cycles                  cycle         4033
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.10
    L1/TEX Cache Throughput             %        28.07
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        32.06
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.19
    Achieved Active Warps Per SM           warp         1.05
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.2%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.08
    SM Frequency            cycle/nsecond         1.26
    Elapsed Cycles                  cycle         3909
    Memory Throughput                   %         0.48
    DRAM Throughput                     %         0.03
    Duration                      usecond         3.10
    L1/TEX Cache Throughput             %        26.48
    L2 Cache Throughput                 %         0.48
    SM Active Cycles                cycle        33.99
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         1.92
    Achieved Active Warps Per SM           warp         0.92
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (1.9%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelLeftBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.61
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle         4300
    Memory Throughput                   %         0.55
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.20
    L1/TEX Cache Throughput             %         3.37
    L2 Cache Throughput                 %         0.55
    SM Active Cycles                cycle       266.84
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelRightBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.23
    SM Frequency            cycle/nsecond         1.28
    Elapsed Cycles                  cycle         4222
    Memory Throughput                   %         0.56
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.30
    L1/TEX Cache Throughput             %         3.34
    L2 Cache Throughput                 %         0.56
    SM Active Cycles                cycle       269.69
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.14
    Achieved Active Warps Per SM           warp         1.03
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelBottomBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.73
    SM Frequency            cycle/nsecond         1.36
    Elapsed Cycles                  cycle         4488
    Memory Throughput                   %         0.63
    DRAM Throughput                     %         0.28
    Duration                      usecond         3.30
    L1/TEX Cache Throughput             %         3.29
    L2 Cache Throughput                 %         0.63
    SM Active Cycles                cycle       280.75
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.10
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelTopBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.77
    SM Frequency            cycle/nsecond         1.37
    Elapsed Cycles                  cycle         4559
    Memory Throughput                   %         0.62
    DRAM Throughput                     %         0.27
    Duration                      usecond         3.33
    L1/TEX Cache Throughput             %         3.33
    L2 Cache Throughput                 %         0.62
    SM Active Cycles                cycle       277.82
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.97
    SM Frequency            cycle/nsecond         1.40
    Elapsed Cycles                  cycle        20066
    Memory Throughput                   %         2.63
    DRAM Throughput                     %         1.33
    Duration                      usecond        14.37
    L1/TEX Cache Throughput             %         2.66
    L2 Cache Throughput                 %         2.63
    SM Active Cycles                cycle     16604.13
    Compute (SM) Throughput             %        43.28
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           18
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (66.7%) and measured achieved occupancy (4.2%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (1, 8, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.64
    SM Frequency            cycle/nsecond         1.35
    Elapsed Cycles                  cycle        10944
    Memory Throughput                   %         0.92
    DRAM Throughput                     %         0.06
    Duration                      usecond         8.13
    L1/TEX Cache Throughput             %         0.91
    L2 Cache Throughput                 %         0.92
    SM Active Cycles                cycle      1059.47
    Compute (SM) Throughput             %         2.49
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 1, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.70
    SM Frequency            cycle/nsecond         1.35
    Elapsed Cycles                  cycle        11142
    Memory Throughput                   %         0.94
    DRAM Throughput                     %         0.11
    Duration                      usecond         8.22
    L1/TEX Cache Throughput             %         1.06
    L2 Cache Throughput                 %         0.94
    SM Active Cycles                cycle      1074.60
    Compute (SM) Throughput             %         2.48
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__reduce::ReduceAgent<thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>, thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>(T2, T3, T4, T5) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.81
    SM Frequency            cycle/nsecond         1.22
    Elapsed Cycles                  cycle         5029
    Memory Throughput                   %         2.84
    DRAM Throughput                     %         0.03
    Duration                      usecond         4.13
    L1/TEX Cache Throughput             %        19.03
    L2 Cache Throughput                 %         2.84
    SM Active Cycles                cycle        47.37
    Compute (SM) Throughput             %         0.15
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.16
    Achieved Active Warps Per SM           warp         6.80
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (14.2%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>, thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.21
    SM Frequency            cycle/nsecond         1.28
    Elapsed Cycles                  cycle         3185
    Memory Throughput                   %         0.66
    DRAM Throughput                     %         0.02
    Duration                      usecond         2.50
    L1/TEX Cache Throughput             %        44.67
    L2 Cache Throughput                 %         0.66
    SM Active Cycles                cycle        20.15
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        12.02
    Achieved Active Warps Per SM           warp         5.77
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>, thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.84
    SM Frequency            cycle/nsecond         1.52
    Elapsed Cycles                  cycle         2586
    Memory Throughput                   %         0.61
    DRAM Throughput                     %            0
    Duration                      usecond         1.70
    L1/TEX Cache Throughput             %       208.16
    L2 Cache Throughput                 %         0.61
    SM Active Cycles                cycle         4.32
    Compute (SM) Throughput             %         0.00
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.12
    Achieved Active Warps Per SM           warp         7.74
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (16.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::updateUnknownsKernel(const double *, const double *, const double *, const double *, const double *, const double *, const double *, const double *, double *, double *, double *, double, double, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.25
    SM Frequency            cycle/nsecond         1.45
    Elapsed Cycles                  cycle         5978
    Memory Throughput                   %        12.04
    DRAM Throughput                     %        12.04
    Duration                      usecond         4.13
    L1/TEX Cache Throughput             %         7.51
    L2 Cache Throughput                 %         9.83
    SM Active Cycles                cycle      3107.72
    Compute (SM) Throughput             %         4.54
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         3.99
    Achieved Active Warps Per SM           warp         1.91
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (66.7%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (66.7%) and measured achieved occupancy (4.0%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.35
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         4018
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.10
    L1/TEX Cache Throughput             %        27.18
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        33.12
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.03
    Achieved Active Warps Per SM           warp         0.98
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.0%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.51
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle         3995
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.02
    Duration                      usecond         3.01
    L1/TEX Cache Throughput             %        28.00
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        32.15
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.23
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         3972
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.07
    L1/TEX Cache Throughput             %        27.28
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        32.99
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         1.97
    Achieved Active Warps Per SM           warp         0.94
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.0%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelLeftBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.46
    SM Frequency            cycle/nsecond         1.32
    Elapsed Cycles                  cycle         4281
    Memory Throughput                   %         0.55
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.23
    L1/TEX Cache Throughput             %         3.35
    L2 Cache Throughput                 %         0.55
    SM Active Cycles                cycle       268.99
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelRightBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.56
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle         4360
    Memory Throughput                   %         0.54
    DRAM Throughput                     %         0.09
    Duration                      usecond         3.26
    L1/TEX Cache Throughput             %         3.25
    L2 Cache Throughput                 %         0.54
    SM Active Cycles                cycle       276.56
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.06
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelBottomBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.27
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         4368
    Memory Throughput                   %         0.64
    DRAM Throughput                     %         0.27
    Duration                      usecond         3.39
    L1/TEX Cache Throughput             %         3.30
    L2 Cache Throughput                 %         0.64
    SM Active Cycles                cycle       280.28
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.06
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelTopBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.67
    SM Frequency            cycle/nsecond         1.35
    Elapsed Cycles                  cycle         4467
    Memory Throughput                   %         0.63
    DRAM Throughput                     %         0.28
    Duration                      usecond         3.30
    L1/TEX Cache Throughput             %         3.31
    L2 Cache Throughput                 %         0.63
    SM Active Cycles                cycle       279.47
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.12
    Achieved Active Warps Per SM           warp         1.02
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.91
    SM Frequency            cycle/nsecond         1.39
    Elapsed Cycles                  cycle        20061
    Memory Throughput                   %         2.63
    DRAM Throughput                     %         1.33
    Duration                      usecond        14.46
    L1/TEX Cache Throughput             %         2.66
    L2 Cache Throughput                 %         2.63
    SM Active Cycles                cycle     16588.72
    Compute (SM) Throughput             %        43.24
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           18
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.15
    Achieved Active Warps Per SM           warp         1.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (66.7%) and measured achieved occupancy (4.2%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (1, 8, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.66
    SM Frequency            cycle/nsecond         1.35
    Elapsed Cycles                  cycle        11010
    Memory Throughput                   %         0.91
    DRAM Throughput                     %         0.06
    Duration                      usecond         8.16
    L1/TEX Cache Throughput             %         0.91
    L2 Cache Throughput                 %         0.91
    SM Active Cycles                cycle      1054.91
    Compute (SM) Throughput             %         2.48
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 1, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.60
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle        11124
    Memory Throughput                   %         0.94
    DRAM Throughput                     %         0.11
    Duration                      usecond         8.29
    L1/TEX Cache Throughput             %         1.06
    L2 Cache Throughput                 %         0.94
    SM Active Cycles                cycle      1072.85
    Compute (SM) Throughput             %         2.49
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__reduce::ReduceAgent<thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>, thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>(T2, T3, T4, T5) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.11
    SM Frequency            cycle/nsecond         1.41
    Elapsed Cycles                  cycle         5877
    Memory Throughput                   %         2.43
    DRAM Throughput                     %         0.03
    Duration                      usecond         4.16
    L1/TEX Cache Throughput             %        18.98
    L2 Cache Throughput                 %         2.43
    SM Active Cycles                cycle        47.50
    Compute (SM) Throughput             %         0.13
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.00
    Achieved Active Warps Per SM           warp         6.72
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (14.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>, thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.92
    SM Frequency            cycle/nsecond         1.24
    Elapsed Cycles                  cycle         3167
    Memory Throughput                   %         0.66
    DRAM Throughput                     %         0.02
    Duration                      usecond         2.56
    L1/TEX Cache Throughput             %        45.10
    L2 Cache Throughput                 %         0.66
    SM Active Cycles                cycle        19.96
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        12.12
    Achieved Active Warps Per SM           warp         5.82
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>, thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.03
    SM Frequency            cycle/nsecond         1.56
    Elapsed Cycles                  cycle         2595
    Memory Throughput                   %         0.60
    DRAM Throughput                     %            0
    Duration                      usecond         1.66
    L1/TEX Cache Throughput             %       222.55
    L2 Cache Throughput                 %         0.60
    SM Active Cycles                cycle         4.04
    Compute (SM) Throughput             %         0.00
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.08
    Achieved Active Warps Per SM           warp         7.72
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (16.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::updateUnknownsKernel(const double *, const double *, const double *, const double *, const double *, const double *, const double *, const double *, double *, double *, double *, double, double, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.70
    SM Frequency            cycle/nsecond         1.51
    Elapsed Cycles                  cycle         6202
    Memory Throughput                   %        11.57
    DRAM Throughput                     %        11.57
    Duration                      usecond         4.10
    L1/TEX Cache Throughput             %         7.72
    L2 Cache Throughput                 %         9.49
    SM Active Cycles                cycle      3023.40
    Compute (SM) Throughput             %         4.37
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.11
    Achieved Active Warps Per SM           warp         1.97
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (66.7%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (66.7%) and measured achieved occupancy (4.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.38
    SM Frequency            cycle/nsecond         1.30
    Elapsed Cycles                  cycle         4047
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.10
    L1/TEX Cache Throughput             %        27.68
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        32.51
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.12
    Achieved Active Warps Per SM           warp         1.02
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.27
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         3972
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.07
    L1/TEX Cache Throughput             %        27.69
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        32.50
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.04
    Achieved Active Warps Per SM           warp         0.98
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.0%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.37
    SM Frequency            cycle/nsecond         1.31
    Elapsed Cycles                  cycle         3940
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.01
    L1/TEX Cache Throughput             %        28.28
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.82
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.06
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelLeftBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.57
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle         4233
    Memory Throughput                   %         0.56
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.17
    L1/TEX Cache Throughput             %         3.31
    L2 Cache Throughput                 %         0.56
    SM Active Cycles                cycle       271.68
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.03
    Achieved Active Warps Per SM           warp         0.98
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.0%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelRightBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.52
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle         4287
    Memory Throughput                   %         0.55
    DRAM Throughput                     %         0.09
    Duration                      usecond         3.23
    L1/TEX Cache Throughput             %         3.32
    L2 Cache Throughput                 %         0.55
    SM Active Cycles                cycle       270.88
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.12
    Achieved Active Warps Per SM           warp         1.02
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelBottomBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.67
    SM Frequency            cycle/nsecond         1.35
    Elapsed Cycles                  cycle         4328
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.28
    Duration                      usecond         3.20
    L1/TEX Cache Throughput             %         3.33
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle       277.74
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelTopBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.96
    SM Frequency            cycle/nsecond         1.40
    Elapsed Cycles                  cycle         4477
    Memory Throughput                   %         0.63
    DRAM Throughput                     %         0.27
    Duration                      usecond         3.20
    L1/TEX Cache Throughput             %         3.30
    L2 Cache Throughput                 %         0.63
    SM Active Cycles                cycle       280.46
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.13
    Achieved Active Warps Per SM           warp         1.02
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.97
    SM Frequency            cycle/nsecond         1.40
    Elapsed Cycles                  cycle        20151
    Memory Throughput                   %         2.62
    DRAM Throughput                     %         1.32
    Duration                      usecond        14.43
    L1/TEX Cache Throughput             %         2.65
    L2 Cache Throughput                 %         2.62
    SM Active Cycles                cycle     16640.49
    Compute (SM) Throughput             %        43.05
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           18
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.15
    Achieved Active Warps Per SM           warp         1.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (66.7%) and measured achieved occupancy (4.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (1, 8, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.53
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle        10938
    Memory Throughput                   %         0.92
    DRAM Throughput                     %         0.06
    Duration                      usecond         8.22
    L1/TEX Cache Throughput             %         0.91
    L2 Cache Throughput                 %         0.92
    SM Active Cycles                cycle      1056.81
    Compute (SM) Throughput             %         2.50
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 1, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.52
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle        11107
    Memory Throughput                   %         0.94
    DRAM Throughput                     %         0.11
    Duration                      usecond         8.35
    L1/TEX Cache Throughput             %         1.06
    L2 Cache Throughput                 %         0.94
    SM Active Cycles                cycle      1074.16
    Compute (SM) Throughput             %         2.49
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__reduce::ReduceAgent<thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>, thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>(T2, T3, T4, T5) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.89
    SM Frequency            cycle/nsecond         1.23
    Elapsed Cycles                  cycle         5072
    Memory Throughput                   %         2.82
    DRAM Throughput                     %         0.03
    Duration                      usecond         4.13
    L1/TEX Cache Throughput             %        18.96
    L2 Cache Throughput                 %         2.82
    SM Active Cycles                cycle        47.54
    Compute (SM) Throughput             %         0.15
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        13.94
    Achieved Active Warps Per SM           warp         6.69
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (13.9%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>, thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.18
    SM Frequency            cycle/nsecond         1.28
    Elapsed Cycles                  cycle         3239
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.02
    Duration                      usecond         2.53
    L1/TEX Cache Throughput             %        44.28
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle        20.32
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        11.75
    Achieved Active Warps Per SM           warp         5.64
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (11.7%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>, thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.96
    SM Frequency            cycle/nsecond         1.55
    Elapsed Cycles                  cycle         2625
    Memory Throughput                   %         0.60
    DRAM Throughput                     %            0
    Duration                      usecond         1.70
    L1/TEX Cache Throughput             %       208.16
    L2 Cache Throughput                 %         0.60
    SM Active Cycles                cycle         4.32
    Compute (SM) Throughput             %         0.00
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.12
    Achieved Active Warps Per SM           warp         7.74
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (16.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::updateUnknownsKernel(const double *, const double *, const double *, const double *, const double *, const double *, const double *, const double *, double *, double *, double *, double, double, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.52
    SM Frequency            cycle/nsecond         1.48
    Elapsed Cycles                  cycle         5935
    Memory Throughput                   %        12.07
    DRAM Throughput                     %        12.07
    Duration                      usecond            4
    L1/TEX Cache Throughput             %         7.73
    L2 Cache Throughput                 %         9.90
    SM Active Cycles                cycle      3019.96
    Compute (SM) Throughput             %         4.57
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.12
    Achieved Active Warps Per SM           warp         1.98
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (66.7%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (66.7%) and measured achieved occupancy (4.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.41
    SM Frequency            cycle/nsecond         1.31
    Elapsed Cycles                  cycle         4074
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.10
    L1/TEX Cache Throughput             %        27.37
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        32.88
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.05
    Achieved Active Warps Per SM           warp         0.98
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.0%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.25
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         3998
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.02
    Duration                      usecond         3.10
    L1/TEX Cache Throughput             %        28.15
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.97
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.32
    SM Frequency            cycle/nsecond         1.30
    Elapsed Cycles                  cycle         3949
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.02
    Duration                      usecond         3.04
    L1/TEX Cache Throughput             %        26.83
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        33.54
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         1.93
    Achieved Active Warps Per SM           warp         0.93
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (1.9%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelLeftBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.39
    SM Frequency            cycle/nsecond         1.31
    Elapsed Cycles                  cycle         4314
    Memory Throughput                   %         0.55
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.30
    L1/TEX Cache Throughput             %         3.35
    L2 Cache Throughput                 %         0.55
    SM Active Cycles                cycle       268.46
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelRightBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.32
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         4331
    Memory Throughput                   %         0.54
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.36
    L1/TEX Cache Throughput             %         3.27
    L2 Cache Throughput                 %         0.54
    SM Active Cycles                cycle       275.63
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelBottomBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.40
    SM Frequency            cycle/nsecond         1.31
    Elapsed Cycles                  cycle         4375
    Memory Throughput                   %         0.64
    DRAM Throughput                     %         0.27
    Duration                      usecond         3.33
    L1/TEX Cache Throughput             %         3.29
    L2 Cache Throughput                 %         0.64
    SM Active Cycles                cycle       281.06
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.05
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelTopBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.24
    SM Frequency            cycle/nsecond         1.28
    Elapsed Cycles                  cycle         4339
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.29
    Duration                      usecond         3.39
    L1/TEX Cache Throughput             %         3.27
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle       282.90
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.11
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.92
    SM Frequency            cycle/nsecond         1.39
    Elapsed Cycles                  cycle        20053
    Memory Throughput                   %         2.63
    DRAM Throughput                     %         1.33
    Duration                      usecond        14.40
    L1/TEX Cache Throughput             %         2.66
    L2 Cache Throughput                 %         2.63
    SM Active Cycles                cycle     16570.28
    Compute (SM) Throughput             %        43.14
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           18
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         1.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (66.7%) and measured achieved occupancy (4.2%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (1, 8, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.57
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle        10905
    Memory Throughput                   %         0.92
    DRAM Throughput                     %         0.06
    Duration                      usecond         8.16
    L1/TEX Cache Throughput             %         0.90
    L2 Cache Throughput                 %         0.92
    SM Active Cycles                cycle      1061.56
    Compute (SM) Throughput             %         2.50
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 1, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.59
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle        11094
    Memory Throughput                   %         0.94
    DRAM Throughput                     %         0.11
    Duration                      usecond         8.29
    L1/TEX Cache Throughput             %         1.06
    L2 Cache Throughput                 %         0.94
    SM Active Cycles                cycle      1072.50
    Compute (SM) Throughput             %         2.49
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__reduce::ReduceAgent<thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>, thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>(T2, T3, T4, T5) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.03
    SM Frequency            cycle/nsecond         1.40
    Elapsed Cycles                  cycle         5828
    Memory Throughput                   %         2.46
    DRAM Throughput                     %         0.03
    Duration                      usecond         4.16
    L1/TEX Cache Throughput             %        18.94
    L2 Cache Throughput                 %         2.46
    SM Active Cycles                cycle        47.59
    Compute (SM) Throughput             %         0.13
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        13.98
    Achieved Active Warps Per SM           warp         6.71
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (14.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>, thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.12
    SM Frequency            cycle/nsecond         1.27
    Elapsed Cycles                  cycle         3160
    Memory Throughput                   %         0.66
    DRAM Throughput                     %         0.01
    Duration                      usecond         2.50
    L1/TEX Cache Throughput             %        44.70
    L2 Cache Throughput                 %         0.66
    SM Active Cycles                cycle        20.13
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        12.02
    Achieved Active Warps Per SM           warp         5.77
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>, thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.38
    SM Frequency            cycle/nsecond         1.63
    Elapsed Cycles                  cycle         2760
    Memory Throughput                   %         0.57
    DRAM Throughput                     %            0
    Duration                      usecond         1.70
    L1/TEX Cache Throughput             %       221.74
    L2 Cache Throughput                 %         0.57
    SM Active Cycles                cycle         4.06
    Compute (SM) Throughput             %         0.00
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.96
    Achieved Active Warps Per SM           warp         7.66
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (16.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::updateUnknownsKernel(const double *, const double *, const double *, const double *, const double *, const double *, const double *, const double *, double *, double *, double *, double, double, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.06
    SM Frequency            cycle/nsecond         1.42
    Elapsed Cycles                  cycle         6171
    Memory Throughput                   %        11.67
    DRAM Throughput                     %        11.67
    Duration                      usecond         4.35
    L1/TEX Cache Throughput             %         7.39
    L2 Cache Throughput                 %         9.52
    SM Active Cycles                cycle      3158.71
    Compute (SM) Throughput             %         4.40
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         3.94
    Achieved Active Warps Per SM           warp         1.89
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (66.7%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (66.7%) and measured achieved occupancy (3.9%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.35
    SM Frequency            cycle/nsecond         1.30
    Elapsed Cycles                  cycle         4040
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.10
    L1/TEX Cache Throughput             %        27.51
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        32.72
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.11
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.25
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         4001
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.02
    Duration                      usecond         3.10
    L1/TEX Cache Throughput             %        27.59
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        32.62
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.05
    Achieved Active Warps Per SM           warp         0.98
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.0%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.35
    SM Frequency            cycle/nsecond         1.30
    Elapsed Cycles                  cycle         3949
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.04
    L1/TEX Cache Throughput             %        28.57
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.50
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.10
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelLeftBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.30
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         4247
    Memory Throughput                   %         0.56
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.30
    L1/TEX Cache Throughput             %         3.41
    L2 Cache Throughput                 %         0.56
    SM Active Cycles                cycle       264.09
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.13
    Achieved Active Warps Per SM           warp         1.02
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelRightBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.23
    SM Frequency            cycle/nsecond         1.28
    Elapsed Cycles                  cycle         4228
    Memory Throughput                   %         0.55
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.30
    L1/TEX Cache Throughput             %         3.31
    L2 Cache Throughput                 %         0.55
    SM Active Cycles                cycle       272.04
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.06
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelBottomBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.32
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         4342
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.27
    Duration                      usecond         3.36
    L1/TEX Cache Throughput             %         3.32
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle          279
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.12
    Achieved Active Warps Per SM           warp         1.02
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelTopBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.38
    SM Frequency            cycle/nsecond         1.31
    Elapsed Cycles                  cycle         4389
    Memory Throughput                   %         0.64
    DRAM Throughput                     %         0.28
    Duration                      usecond         3.36
    L1/TEX Cache Throughput             %         3.32
    L2 Cache Throughput                 %         0.64
    SM Active Cycles                cycle       278.75
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.96
    SM Frequency            cycle/nsecond         1.40
    Elapsed Cycles                  cycle        20048
    Memory Throughput                   %         2.63
    DRAM Throughput                     %         1.33
    Duration                      usecond        14.34
    L1/TEX Cache Throughput             %         2.66
    L2 Cache Throughput                 %         2.63
    SM Active Cycles                cycle     16633.74
    Compute (SM) Throughput             %        42.96
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           18
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.13
    Achieved Active Warps Per SM           warp         1.98
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (66.7%) and measured achieved occupancy (4.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (1, 8, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.67
    SM Frequency            cycle/nsecond         1.35
    Elapsed Cycles                  cycle        10948
    Memory Throughput                   %         0.91
    DRAM Throughput                     %         0.06
    Duration                      usecond         8.13
    L1/TEX Cache Throughput             %         0.91
    L2 Cache Throughput                 %         0.91
    SM Active Cycles                cycle      1056.41
    Compute (SM) Throughput             %         2.49
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 1, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.63
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle        11103
    Memory Throughput                   %         0.94
    DRAM Throughput                     %         0.11
    Duration                      usecond         8.26
    L1/TEX Cache Throughput             %         1.06
    L2 Cache Throughput                 %         0.94
    SM Active Cycles                cycle      1077.34
    Compute (SM) Throughput             %         2.49
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__reduce::ReduceAgent<thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>, thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>(T2, T3, T4, T5) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.83
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle         5040
    Memory Throughput                   %         2.84
    DRAM Throughput                     %         0.03
    Duration                      usecond         4.16
    L1/TEX Cache Throughput             %        18.77
    L2 Cache Throughput                 %         2.84
    SM Active Cycles                cycle        48.01
    Compute (SM) Throughput             %         0.15
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        13.96
    Achieved Active Warps Per SM           warp         6.70
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (14.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>, thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.38
    SM Frequency            cycle/nsecond         1.31
    Elapsed Cycles                  cycle         3185
    Memory Throughput                   %         0.66
    DRAM Throughput                     %         0.02
    Duration                      usecond         2.43
    L1/TEX Cache Throughput             %           45
    L2 Cache Throughput                 %         0.66
    SM Active Cycles                cycle           20
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        12.12
    Achieved Active Warps Per SM           warp         5.82
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>, thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.96
    SM Frequency            cycle/nsecond         1.56
    Elapsed Cycles                  cycle         2655
    Memory Throughput                   %         0.59
    DRAM Throughput                     %            0
    Duration                      usecond         1.70
    L1/TEX Cache Throughput             %       208.16
    L2 Cache Throughput                 %         0.59
    SM Active Cycles                cycle         4.32
    Compute (SM) Throughput             %         0.00
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.18
    Achieved Active Warps Per SM           warp         7.77
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (16.2%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::updateUnknownsKernel(const double *, const double *, const double *, const double *, const double *, const double *, const double *, const double *, double *, double *, double *, double, double, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.70
    SM Frequency            cycle/nsecond         1.50
    Elapsed Cycles                  cycle         6119
    Memory Throughput                   %        11.66
    DRAM Throughput                     %        11.66
    Duration                      usecond         4.06
    L1/TEX Cache Throughput             %         7.27
    L2 Cache Throughput                 %         9.62
    SM Active Cycles                cycle      3210.81
    Compute (SM) Throughput             %         4.43
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.01
    Achieved Active Warps Per SM           warp         1.92
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (66.7%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (66.7%) and measured achieved occupancy (4.0%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.59
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle         4069
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.02
    Duration                      usecond         3.04
    L1/TEX Cache Throughput             %        27.59
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        32.62
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.10
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.45
    SM Frequency            cycle/nsecond         1.32
    Elapsed Cycles                  cycle         4024
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.04
    L1/TEX Cache Throughput             %        28.19
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        31.93
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.10
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.57
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle         3983
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.23
    Duration                      usecond         2.98
    L1/TEX Cache Throughput             %        28.83
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.22
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.19
    Achieved Active Warps Per SM           warp         1.05
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.2%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelLeftBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.49
    SM Frequency            cycle/nsecond         1.32
    Elapsed Cycles                  cycle         4254
    Memory Throughput                   %         0.56
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.23
    L1/TEX Cache Throughput             %         3.35
    L2 Cache Throughput                 %         0.56
    SM Active Cycles                cycle       268.68
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelRightBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.43
    SM Frequency            cycle/nsecond         1.31
    Elapsed Cycles                  cycle         4366
    Memory Throughput                   %         0.54
    DRAM Throughput                     %         0.09
    Duration                      usecond         3.33
    L1/TEX Cache Throughput             %         3.29
    L2 Cache Throughput                 %         0.54
    SM Active Cycles                cycle       273.38
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.12
    Achieved Active Warps Per SM           warp         1.02
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelBottomBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.48
    SM Frequency            cycle/nsecond         1.31
    Elapsed Cycles                  cycle         4331
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.29
    Duration                      usecond         3.30
    L1/TEX Cache Throughput             %         3.32
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle       278.25
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.10
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelTopBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.29
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         4329
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.28
    Duration                      usecond         3.36
    L1/TEX Cache Throughput             %         3.29
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle       281.04
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.06
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.88
    SM Frequency            cycle/nsecond         1.38
    Elapsed Cycles                  cycle        20005
    Memory Throughput                   %         2.64
    DRAM Throughput                     %         1.33
    Duration                      usecond        14.46
    L1/TEX Cache Throughput             %         2.67
    L2 Cache Throughput                 %         2.64
    SM Active Cycles                cycle     16512.87
    Compute (SM) Throughput             %        42.91
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           18
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (66.7%) and measured achieved occupancy (4.2%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (1, 8, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.55
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle        10905
    Memory Throughput                   %         0.92
    DRAM Throughput                     %         0.06
    Duration                      usecond         8.19
    L1/TEX Cache Throughput             %         0.91
    L2 Cache Throughput                 %         0.92
    SM Active Cycles                cycle      1057.16
    Compute (SM) Throughput             %         2.50
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 1, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.62
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle        11083
    Memory Throughput                   %         0.94
    DRAM Throughput                     %         0.11
    Duration                      usecond         8.26
    L1/TEX Cache Throughput             %         1.05
    L2 Cache Throughput                 %         0.94
    SM Active Cycles                cycle      1079.37
    Compute (SM) Throughput             %         2.50
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__reduce::ReduceAgent<thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>, thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>(T2, T3, T4, T5) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.00
    SM Frequency            cycle/nsecond         1.40
    Elapsed Cycles                  cycle         5774
    Memory Throughput                   %         2.48
    DRAM Throughput                     %         0.03
    Duration                      usecond         4.13
    L1/TEX Cache Throughput             %        19.02
    L2 Cache Throughput                 %         2.48
    SM Active Cycles                cycle        47.40
    Compute (SM) Throughput             %         0.13
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.10
    Achieved Active Warps Per SM           warp         6.77
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (14.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>, thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.21
    SM Frequency            cycle/nsecond         1.28
    Elapsed Cycles                  cycle         3191
    Memory Throughput                   %         0.75
    DRAM Throughput                     %         0.02
    Duration                      usecond         2.50
    L1/TEX Cache Throughput             %        45.03
    L2 Cache Throughput                 %         0.75
    SM Active Cycles                cycle        19.99
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        12.01
    Achieved Active Warps Per SM           warp         5.76
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>, thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.75
    SM Frequency            cycle/nsecond         1.67
    Elapsed Cycles                  cycle         2678
    Memory Throughput                   %         0.58
    DRAM Throughput                     %            0
    Duration                      usecond         1.60
    L1/TEX Cache Throughput             %       222.55
    L2 Cache Throughput                 %         0.58
    SM Active Cycles                cycle         4.04
    Compute (SM) Throughput             %         0.00
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.08
    Achieved Active Warps Per SM           warp         7.72
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (16.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::updateUnknownsKernel(const double *, const double *, const double *, const double *, const double *, const double *, const double *, const double *, double *, double *, double *, double, double, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.39
    SM Frequency            cycle/nsecond         1.61
    Elapsed Cycles                  cycle         6464
    Memory Throughput                   %        11.06
    DRAM Throughput                     %        11.06
    Duration                      usecond            4
    L1/TEX Cache Throughput             %         7.71
    L2 Cache Throughput                 %         9.07
    SM Active Cycles                cycle      3028.75
    Compute (SM) Throughput             %         4.20
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.08
    Achieved Active Warps Per SM           warp         1.96
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (66.7%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (66.7%) and measured achieved occupancy (4.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.52
    SM Frequency            cycle/nsecond         1.32
    Elapsed Cycles                  cycle         4022
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.04
    L1/TEX Cache Throughput             %        27.54
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        32.68
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.10
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.28
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         3995
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.10
    L1/TEX Cache Throughput             %        27.22
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        33.06
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.03
    Achieved Active Warps Per SM           warp         0.97
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.0%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.41
    SM Frequency            cycle/nsecond         1.31
    Elapsed Cycles                  cycle         3937
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.01
    L1/TEX Cache Throughput             %        28.58
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.49
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelLeftBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.65
    SM Frequency            cycle/nsecond         1.35
    Elapsed Cycles                  cycle         4219
    Memory Throughput                   %         0.56
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.14
    L1/TEX Cache Throughput             %         3.34
    L2 Cache Throughput                 %         0.56
    SM Active Cycles                cycle       269.63
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.10
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelRightBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.66
    SM Frequency            cycle/nsecond         1.35
    Elapsed Cycles                  cycle         4278
    Memory Throughput                   %         0.55
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.17
    L1/TEX Cache Throughput             %         3.32
    L2 Cache Throughput                 %         0.55
    SM Active Cycles                cycle       271.19
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelBottomBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.59
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle         4333
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.29
    Duration                      usecond         3.23
    L1/TEX Cache Throughput             %         3.23
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle       286.78
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.04
    Achieved Active Warps Per SM           warp         0.98
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.0%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelTopBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.64
    SM Frequency            cycle/nsecond         1.35
    Elapsed Cycles                  cycle         4312
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.30
    Duration                      usecond         3.20
    L1/TEX Cache Throughput             %         3.29
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle       281.31
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.88
    SM Frequency            cycle/nsecond         1.38
    Elapsed Cycles                  cycle        20028
    Memory Throughput                   %         2.63
    DRAM Throughput                     %         1.33
    Duration                      usecond        14.46
    L1/TEX Cache Throughput             %         2.67
    L2 Cache Throughput                 %         2.63
    SM Active Cycles                cycle     16519.35
    Compute (SM) Throughput             %        42.80
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           18
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.17
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (66.7%) and measured achieved occupancy (4.2%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (1, 8, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.54
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle        10954
    Memory Throughput                   %         0.91
    DRAM Throughput                     %         0.06
    Duration                      usecond         8.22
    L1/TEX Cache Throughput             %         0.91
    L2 Cache Throughput                 %         0.91
    SM Active Cycles                cycle      1056.32
    Compute (SM) Throughput             %         2.49
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 1, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.55
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle        11096
    Memory Throughput                   %         0.94
    DRAM Throughput                     %         0.11
    Duration                      usecond         8.32
    L1/TEX Cache Throughput             %         1.06
    L2 Cache Throughput                 %         0.94
    SM Active Cycles                cycle      1076.69
    Compute (SM) Throughput             %         2.49
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__reduce::ReduceAgent<thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>, thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>(T2, T3, T4, T5) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.03
    SM Frequency            cycle/nsecond         1.41
    Elapsed Cycles                  cycle         5879
    Memory Throughput                   %         2.44
    DRAM Throughput                     %         0.03
    Duration                      usecond         4.16
    L1/TEX Cache Throughput             %        18.79
    L2 Cache Throughput                 %         2.44
    SM Active Cycles                cycle        47.99
    Compute (SM) Throughput             %         0.13
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.09
    Achieved Active Warps Per SM           warp         6.76
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (14.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>, thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.08
    SM Frequency            cycle/nsecond         1.27
    Elapsed Cycles                  cycle         3171
    Memory Throughput                   %         0.66
    DRAM Throughput                     %         0.01
    Duration                      usecond         2.50
    L1/TEX Cache Throughput             %        45.10
    L2 Cache Throughput                 %         0.66
    SM Active Cycles                cycle        19.96
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        11.97
    Achieved Active Warps Per SM           warp         5.74
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>, thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.08
    SM Frequency            cycle/nsecond         1.58
    Elapsed Cycles                  cycle         2676
    Memory Throughput                   %         0.58
    DRAM Throughput                     %            0
    Duration                      usecond         1.70
    L1/TEX Cache Throughput             %       208.16
    L2 Cache Throughput                 %         0.58
    SM Active Cycles                cycle         4.32
    Compute (SM) Throughput             %         0.00
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.18
    Achieved Active Warps Per SM           warp         7.77
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (16.2%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::updateUnknownsKernel(const double *, const double *, const double *, const double *, const double *, const double *, const double *, const double *, double *, double *, double *, double, double, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.14
    SM Frequency            cycle/nsecond         1.42
    Elapsed Cycles                  cycle         5739
    Memory Throughput                   %        12.47
    DRAM Throughput                     %        12.47
    Duration                      usecond         4.03
    L1/TEX Cache Throughput             %         7.43
    L2 Cache Throughput                 %        10.23
    SM Active Cycles                cycle      3142.82
    Compute (SM) Throughput             %         4.73
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.01
    Achieved Active Warps Per SM           warp         1.92
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (66.7%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (66.7%) and measured achieved occupancy (4.0%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.31
    SM Frequency            cycle/nsecond         1.30
    Elapsed Cycles                  cycle         4048
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.10
    L1/TEX Cache Throughput             %        27.56
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        32.66
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.21
    Achieved Active Warps Per SM           warp         1.06
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.2%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.25
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         3998
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.10
    L1/TEX Cache Throughput             %        28.05
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        32.09
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.23
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         3958
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.07
    L1/TEX Cache Throughput             %        28.75
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.31
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelLeftBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.99
    SM Frequency            cycle/nsecond         1.25
    Elapsed Cycles                  cycle         4265
    Memory Throughput                   %         0.63
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.42
    L1/TEX Cache Throughput             %         3.33
    L2 Cache Throughput                 %         0.63
    SM Active Cycles                cycle       270.40
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelRightBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.03
    SM Frequency            cycle/nsecond         1.25
    Elapsed Cycles                  cycle         4326
    Memory Throughput                   %         0.54
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.46
    L1/TEX Cache Throughput             %         3.26
    L2 Cache Throughput                 %         0.54
    SM Active Cycles                cycle       275.94
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelBottomBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.45
    SM Frequency            cycle/nsecond         1.32
    Elapsed Cycles                  cycle         4472
    Memory Throughput                   %         0.71
    DRAM Throughput                     %         0.28
    Duration                      usecond         3.39
    L1/TEX Cache Throughput             %         3.30
    L2 Cache Throughput                 %         0.71
    SM Active Cycles                cycle       280.53
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.06
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelTopBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.32
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         4350
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.29
    Duration                      usecond         3.36
    L1/TEX Cache Throughput             %         3.13
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle       295.96
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.10
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.04
    SM Frequency            cycle/nsecond         1.41
    Elapsed Cycles                  cycle        20223
    Memory Throughput                   %         2.61
    DRAM Throughput                     %         1.32
    Duration                      usecond        14.37
    L1/TEX Cache Throughput             %         2.64
    L2 Cache Throughput                 %         2.61
    SM Active Cycles                cycle     16368.76
    Compute (SM) Throughput             %        42.07
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           18
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (66.7%) and measured achieved occupancy (4.2%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (1, 8, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.56
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle        10920
    Memory Throughput                   %         0.92
    DRAM Throughput                     %         0.06
    Duration                      usecond         8.19
    L1/TEX Cache Throughput             %         0.91
    L2 Cache Throughput                 %         0.92
    SM Active Cycles                cycle      1055.90
    Compute (SM) Throughput             %         2.50
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 1, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.62
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle        11080
    Memory Throughput                   %         0.94
    DRAM Throughput                     %         0.11
    Duration                      usecond         8.26
    L1/TEX Cache Throughput             %         1.06
    L2 Cache Throughput                 %         0.94
    SM Active Cycles                cycle      1073.38
    Compute (SM) Throughput             %         2.50
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__reduce::ReduceAgent<thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>, thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>(T2, T3, T4, T5) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.75
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle         5044
    Memory Throughput                   %         2.83
    DRAM Throughput                     %         0.03
    Duration                      usecond         4.16
    L1/TEX Cache Throughput             %        18.87
    L2 Cache Throughput                 %         2.83
    SM Active Cycles                cycle        47.78
    Compute (SM) Throughput             %         0.15
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.03
    Achieved Active Warps Per SM           warp         6.73
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (14.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>, thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.38
    SM Frequency            cycle/nsecond         1.30
    Elapsed Cycles                  cycle         3160
    Memory Throughput                   %         0.66
    DRAM Throughput                     %         0.01
    Duration                      usecond         2.43
    L1/TEX Cache Throughput             %        44.80
    L2 Cache Throughput                 %         0.66
    SM Active Cycles                cycle        20.09
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        12.01
    Achieved Active Warps Per SM           warp         5.77
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>, thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.40
    SM Frequency            cycle/nsecond         1.62
    Elapsed Cycles                  cycle         2692
    Memory Throughput                   %         0.58
    DRAM Throughput                     %            0
    Duration                      usecond         1.66
    L1/TEX Cache Throughput             %       222.55
    L2 Cache Throughput                 %         0.58
    SM Active Cycles                cycle         4.04
    Compute (SM) Throughput             %         0.00
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.02
    Achieved Active Warps Per SM           warp         7.69
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (16.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::updateUnknownsKernel(const double *, const double *, const double *, const double *, const double *, const double *, const double *, const double *, double *, double *, double *, double, double, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.62
    SM Frequency            cycle/nsecond         1.50
    Elapsed Cycles                  cycle         6156
    Memory Throughput                   %        11.66
    DRAM Throughput                     %        11.66
    Duration                      usecond         4.10
    L1/TEX Cache Throughput             %         7.71
    L2 Cache Throughput                 %         9.55
    SM Active Cycles                cycle      3027.01
    Compute (SM) Throughput             %         4.41
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.10
    Achieved Active Warps Per SM           warp         1.97
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (66.7%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (66.7%) and measured achieved occupancy (4.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.40
    SM Frequency            cycle/nsecond         1.32
    Elapsed Cycles                  cycle         4044
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.07
    L1/TEX Cache Throughput             %        27.68
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        32.51
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.11
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.33
    SM Frequency            cycle/nsecond         1.30
    Elapsed Cycles                  cycle         4001
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.07
    L1/TEX Cache Throughput             %        28.03
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        32.10
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.33
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         3966
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.07
    L1/TEX Cache Throughput             %        29.00
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.03
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.10
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelLeftBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.15
    SM Frequency            cycle/nsecond         1.27
    Elapsed Cycles                  cycle         4243
    Memory Throughput                   %         0.56
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.33
    L1/TEX Cache Throughput             %         3.36
    L2 Cache Throughput                 %         0.56
    SM Active Cycles                cycle       268.13
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelRightBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.93
    SM Frequency            cycle/nsecond         1.23
    Elapsed Cycles                  cycle         4222
    Memory Throughput                   %         0.56
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.42
    L1/TEX Cache Throughput             %         3.33
    L2 Cache Throughput                 %         0.56
    SM Active Cycles                cycle          270
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelBottomBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.46
    SM Frequency            cycle/nsecond         1.31
    Elapsed Cycles                  cycle         4358
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.28
    Duration                      usecond         3.33
    L1/TEX Cache Throughput             %         3.35
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle       276.18
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.11
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelTopBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.75
    SM Frequency            cycle/nsecond         1.51
    Elapsed Cycles                  cycle         5333
    Memory Throughput                   %         0.53
    DRAM Throughput                     %         0.23
    Duration                      usecond         3.52
    L1/TEX Cache Throughput             %         3.32
    L2 Cache Throughput                 %         0.53
    SM Active Cycles                cycle       278.49
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.11
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond            9
    SM Frequency            cycle/nsecond         1.40
    Elapsed Cycles                  cycle        20102
    Memory Throughput                   %         2.62
    DRAM Throughput                     %         1.33
    Duration                      usecond        14.34
    L1/TEX Cache Throughput             %         2.66
    L2 Cache Throughput                 %         2.62
    SM Active Cycles                cycle     16178.53
    Compute (SM) Throughput             %        42.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           18
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (66.7%) and measured achieved occupancy (4.2%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (1, 8, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.63
    SM Frequency            cycle/nsecond         1.35
    Elapsed Cycles                  cycle        10959
    Memory Throughput                   %         0.91
    DRAM Throughput                     %         0.06
    Duration                      usecond         8.13
    L1/TEX Cache Throughput             %         0.91
    L2 Cache Throughput                 %         0.91
    SM Active Cycles                cycle      1055.94
    Compute (SM) Throughput             %         2.49
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 1, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.68
    SM Frequency            cycle/nsecond         1.35
    Elapsed Cycles                  cycle        11125
    Memory Throughput                   %         0.94
    DRAM Throughput                     %         0.11
    Duration                      usecond         8.22
    L1/TEX Cache Throughput             %         1.06
    L2 Cache Throughput                 %         0.94
    SM Active Cycles                cycle      1074.19
    Compute (SM) Throughput             %         2.49
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__reduce::ReduceAgent<thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>, thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>(T2, T3, T4, T5) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.74
    SM Frequency            cycle/nsecond         1.20
    Elapsed Cycles                  cycle         5034
    Memory Throughput                   %         2.84
    DRAM Throughput                     %         0.03
    Duration                      usecond         4.19
    L1/TEX Cache Throughput             %        18.69
    L2 Cache Throughput                 %         2.84
    SM Active Cycles                cycle        48.24
    Compute (SM) Throughput             %         0.15
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        13.86
    Achieved Active Warps Per SM           warp         6.65
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (13.9%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>, thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.23
    SM Frequency            cycle/nsecond         1.28
    Elapsed Cycles                  cycle         3156
    Memory Throughput                   %         0.67
    DRAM Throughput                     %         0.01
    Duration                      usecond         2.46
    L1/TEX Cache Throughput             %        44.74
    L2 Cache Throughput                 %         0.67
    SM Active Cycles                cycle        20.12
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        11.89
    Achieved Active Warps Per SM           warp         5.71
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (11.9%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>, thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.34
    SM Frequency            cycle/nsecond         1.60
    Elapsed Cycles                  cycle         2664
    Memory Throughput                   %         0.59
    DRAM Throughput                     %            0
    Duration                      usecond         1.66
    L1/TEX Cache Throughput             %       208.87
    L2 Cache Throughput                 %         0.59
    SM Active Cycles                cycle         4.31
    Compute (SM) Throughput             %         0.00
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.23
    Achieved Active Warps Per SM           warp         7.79
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (16.2%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::updateUnknownsKernel(const double *, const double *, const double *, const double *, const double *, const double *, const double *, const double *, double *, double *, double *, double, double, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.38
    SM Frequency            cycle/nsecond         1.46
    Elapsed Cycles                  cycle         5982
    Memory Throughput                   %        11.97
    DRAM Throughput                     %        11.97
    Duration                      usecond         4.10
    L1/TEX Cache Throughput             %         7.28
    L2 Cache Throughput                 %         9.81
    SM Active Cycles                cycle      3204.24
    Compute (SM) Throughput             %         4.54
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         3.95
    Achieved Active Warps Per SM           warp         1.90
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (66.7%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (66.7%) and measured achieved occupancy (3.9%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.35
    SM Frequency            cycle/nsecond         1.30
    Elapsed Cycles                  cycle         4026
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.10
    L1/TEX Cache Throughput             %        27.69
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        32.50
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.24
    Achieved Active Warps Per SM           warp         1.07
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.2%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.21
    SM Frequency            cycle/nsecond         1.28
    Elapsed Cycles                  cycle         3976
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.10
    L1/TEX Cache Throughput             %        28.36
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.74
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.35
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         3935
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.03
    Duration                      usecond         3.04
    L1/TEX Cache Throughput             %        28.73
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.32
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelLeftBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.42
    SM Frequency            cycle/nsecond         1.31
    Elapsed Cycles                  cycle         4206
    Memory Throughput                   %         0.56
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.20
    L1/TEX Cache Throughput             %         3.35
    L2 Cache Throughput                 %         0.56
    SM Active Cycles                cycle       268.96
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelRightBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.53
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle         4354
    Memory Throughput                   %         0.54
    DRAM Throughput                     %         0.07
    Duration                      usecond         3.26
    L1/TEX Cache Throughput             %         3.30
    L2 Cache Throughput                 %         0.54
    SM Active Cycles                cycle       272.40
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.11
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelBottomBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.48
    SM Frequency            cycle/nsecond         1.32
    Elapsed Cycles                  cycle         4344
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.29
    Duration                      usecond         3.30
    L1/TEX Cache Throughput             %         3.31
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle       279.15
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelTopBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.26
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         4344
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.28
    Duration                      usecond         3.36
    L1/TEX Cache Throughput             %         3.30
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle       280.38
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.90
    SM Frequency            cycle/nsecond         1.39
    Elapsed Cycles                  cycle        20043
    Memory Throughput                   %         2.63
    DRAM Throughput                     %         1.33
    Duration                      usecond        14.46
    L1/TEX Cache Throughput             %         2.66
    L2 Cache Throughput                 %         2.63
    SM Active Cycles                cycle     16108.06
    Compute (SM) Throughput             %        42.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           18
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.17
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (66.7%) and measured achieved occupancy (4.2%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (1, 8, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.63
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle        10930
    Memory Throughput                   %         0.92
    DRAM Throughput                     %         0.05
    Duration                      usecond         8.13
    L1/TEX Cache Throughput             %         0.91
    L2 Cache Throughput                 %         0.92
    SM Active Cycles                cycle      1056.32
    Compute (SM) Throughput             %         2.50
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 1, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.67
    SM Frequency            cycle/nsecond         1.35
    Elapsed Cycles                  cycle        11121
    Memory Throughput                   %         0.94
    DRAM Throughput                     %         0.11
    Duration                      usecond         8.22
    L1/TEX Cache Throughput             %         1.06
    L2 Cache Throughput                 %         0.94
    SM Active Cycles                cycle      1075.34
    Compute (SM) Throughput             %         2.49
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__reduce::ReduceAgent<thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>, thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>(T2, T3, T4, T5) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.79
    SM Frequency            cycle/nsecond         1.37
    Elapsed Cycles                  cycle         5753
    Memory Throughput                   %         2.49
    DRAM Throughput                     %         0.03
    Duration                      usecond         4.19
    L1/TEX Cache Throughput             %        18.89
    L2 Cache Throughput                 %         2.49
    SM Active Cycles                cycle        47.72
    Compute (SM) Throughput             %         0.13
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.06
    Achieved Active Warps Per SM           warp         6.75
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (14.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>, thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.10
    SM Frequency            cycle/nsecond         1.26
    Elapsed Cycles                  cycle         3197
    Memory Throughput                   %         0.66
    DRAM Throughput                     %         0.02
    Duration                      usecond         2.53
    L1/TEX Cache Throughput             %        45.03
    L2 Cache Throughput                 %         0.66
    SM Active Cycles                cycle        19.99
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        11.96
    Achieved Active Warps Per SM           warp         5.74
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>, thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.28
    SM Frequency            cycle/nsecond         1.62
    Elapsed Cycles                  cycle         2690
    Memory Throughput                   %         0.58
    DRAM Throughput                     %            0
    Duration                      usecond         1.66
    L1/TEX Cache Throughput             %       223.36
    L2 Cache Throughput                 %         0.58
    SM Active Cycles                cycle         4.03
    Compute (SM) Throughput             %         0.00
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.20
    Achieved Active Warps Per SM           warp         7.78
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (16.2%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::updateUnknownsKernel(const double *, const double *, const double *, const double *, const double *, const double *, const double *, const double *, double *, double *, double *, double, double, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.89
    SM Frequency            cycle/nsecond         1.39
    Elapsed Cycles                  cycle         5816
    Memory Throughput                   %        12.34
    DRAM Throughput                     %        12.34
    Duration                      usecond         4.19
    L1/TEX Cache Throughput             %         7.42
    L2 Cache Throughput                 %        10.10
    SM Active Cycles                cycle      3145.59
    Compute (SM) Throughput             %         4.67
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         3.95
    Achieved Active Warps Per SM           warp         1.90
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (66.7%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (66.7%) and measured achieved occupancy (3.9%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.56
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle         4062
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.02
    Duration                      usecond         3.04
    L1/TEX Cache Throughput             %        27.04
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        33.28
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.41
    SM Frequency            cycle/nsecond         1.32
    Elapsed Cycles                  cycle         3979
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.01
    L1/TEX Cache Throughput             %        28.01
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        32.13
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.60
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle         3975
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.01
    Duration                      usecond         2.98
    L1/TEX Cache Throughput             %        29.05
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        30.99
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.11
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelLeftBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.43
    SM Frequency            cycle/nsecond         1.31
    Elapsed Cycles                  cycle         4244
    Memory Throughput                   %         0.56
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.23
    L1/TEX Cache Throughput             %         3.36
    L2 Cache Throughput                 %         0.56
    SM Active Cycles                cycle       267.57
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelRightBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.74
    SM Frequency            cycle/nsecond         1.35
    Elapsed Cycles                  cycle         4329
    Memory Throughput                   %         0.54
    DRAM Throughput                     %         0.09
    Duration                      usecond         3.20
    L1/TEX Cache Throughput             %         3.33
    L2 Cache Throughput                 %         0.54
    SM Active Cycles                cycle       270.26
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelBottomBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.45
    SM Frequency            cycle/nsecond         1.31
    Elapsed Cycles                  cycle         4320
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.29
    Duration                      usecond         3.30
    L1/TEX Cache Throughput             %         3.33
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle       277.65
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelTopBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.23
    SM Frequency            cycle/nsecond         1.28
    Elapsed Cycles                  cycle         4312
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.28
    Duration                      usecond         3.36
    L1/TEX Cache Throughput             %         3.27
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle       282.76
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.04
    Achieved Active Warps Per SM           warp         0.98
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.0%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.95
    SM Frequency            cycle/nsecond         1.39
    Elapsed Cycles                  cycle        20040
    Memory Throughput                   %         2.63
    DRAM Throughput                     %         1.33
    Duration                      usecond        14.40
    L1/TEX Cache Throughput             %         2.66
    L2 Cache Throughput                 %         2.63
    SM Active Cycles                cycle     16133.66
    Compute (SM) Throughput             %        42.05
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           18
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.15
    Achieved Active Warps Per SM           warp         1.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (66.7%) and measured achieved occupancy (4.2%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (1, 8, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.58
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle        11051
    Memory Throughput                   %         0.91
    DRAM Throughput                     %         0.05
    Duration                      usecond         8.26
    L1/TEX Cache Throughput             %         0.90
    L2 Cache Throughput                 %         0.91
    SM Active Cycles                cycle      1061.82
    Compute (SM) Throughput             %         2.47
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 1, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.61
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle        11130
    Memory Throughput                   %         0.94
    DRAM Throughput                     %         0.18
    Duration                      usecond         8.29
    L1/TEX Cache Throughput             %         1.06
    L2 Cache Throughput                 %         0.94
    SM Active Cycles                cycle      1074.63
    Compute (SM) Throughput             %         2.49
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__reduce::ReduceAgent<thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>, thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>(T2, T3, T4, T5) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.50
    SM Frequency            cycle/nsecond         1.32
    Elapsed Cycles                  cycle         5527
    Memory Throughput                   %         2.59
    DRAM Throughput                     %         0.03
    Duration                      usecond         4.19
    L1/TEX Cache Throughput             %        18.75
    L2 Cache Throughput                 %         2.59
    SM Active Cycles                cycle        48.07
    Compute (SM) Throughput             %         0.14
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.02
    Achieved Active Warps Per SM           warp         6.73
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (14.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>, thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond            8
    SM Frequency            cycle/nsecond         1.24
    Elapsed Cycles                  cycle         3186
    Memory Throughput                   %         0.66
    DRAM Throughput                     %         0.01
    Duration                      usecond         2.56
    L1/TEX Cache Throughput             %        43.90
    L2 Cache Throughput                 %         0.66
    SM Active Cycles                cycle        20.50
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        11.65
    Achieved Active Warps Per SM           warp         5.59
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (11.7%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>, thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.77
    SM Frequency            cycle/nsecond         1.51
    Elapsed Cycles                  cycle         2655
    Memory Throughput                   %         0.59
    DRAM Throughput                     %            0
    Duration                      usecond         1.76
    L1/TEX Cache Throughput             %       207.46
    L2 Cache Throughput                 %         0.59
    SM Active Cycles                cycle         4.34
    Compute (SM) Throughput             %         0.00
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.12
    Achieved Active Warps Per SM           warp         7.74
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (16.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::updateUnknownsKernel(const double *, const double *, const double *, const double *, const double *, const double *, const double *, const double *, double *, double *, double *, double, double, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.10
    SM Frequency            cycle/nsecond         1.42
    Elapsed Cycles                  cycle         5764
    Memory Throughput                   %        12.43
    DRAM Throughput                     %        12.43
    Duration                      usecond         4.06
    L1/TEX Cache Throughput             %         7.69
    L2 Cache Throughput                 %        10.18
    SM Active Cycles                cycle      3033.49
    Compute (SM) Throughput             %         4.71
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.11
    Achieved Active Warps Per SM           warp         1.97
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (66.7%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (66.7%) and measured achieved occupancy (4.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.11
    SM Frequency            cycle/nsecond         1.26
    Elapsed Cycles                  cycle         4007
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.02
    Duration                      usecond         3.17
    L1/TEX Cache Throughput             %        27.48
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        32.75
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.16
    SM Frequency            cycle/nsecond         1.27
    Elapsed Cycles                  cycle         3976
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.14
    L1/TEX Cache Throughput             %        28.50
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.57
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.20
    SM Frequency            cycle/nsecond         1.28
    Elapsed Cycles                  cycle         3930
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.02
    Duration                      usecond         3.07
    L1/TEX Cache Throughput             %        28.90
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.15
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelLeftBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.08
    SM Frequency            cycle/nsecond         1.26
    Elapsed Cycles                  cycle         4238
    Memory Throughput                   %         0.56
    DRAM Throughput                     %         0.09
    Duration                      usecond         3.36
    L1/TEX Cache Throughput             %         3.36
    L2 Cache Throughput                 %         0.56
    SM Active Cycles                cycle       268.04
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelRightBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.29
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         4341
    Memory Throughput                   %         0.54
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.36
    L1/TEX Cache Throughput             %         3.15
    L2 Cache Throughput                 %         0.54
    SM Active Cycles                cycle       285.60
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.02
    Achieved Active Warps Per SM           warp         0.97
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.0%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelBottomBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.37
    SM Frequency            cycle/nsecond         1.31
    Elapsed Cycles                  cycle         4354
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.28
    Duration                      usecond         3.33
    L1/TEX Cache Throughput             %         3.31
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle       279.32
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelTopBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.62
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle         4508
    Memory Throughput                   %         0.62
    DRAM Throughput                     %         0.27
    Duration                      usecond         3.36
    L1/TEX Cache Throughput             %         3.33
    L2 Cache Throughput                 %         0.62
    SM Active Cycles                cycle       278.09
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.85
    SM Frequency            cycle/nsecond         1.38
    Elapsed Cycles                  cycle        20014
    Memory Throughput                   %         2.64
    DRAM Throughput                     %         1.33
    Duration                      usecond        14.53
    L1/TEX Cache Throughput             %         2.67
    L2 Cache Throughput                 %         2.64
    SM Active Cycles                cycle     16075.07
    Compute (SM) Throughput             %        41.90
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           18
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (66.7%) and measured achieved occupancy (4.2%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (1, 8, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.62
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle        11015
    Memory Throughput                   %         0.91
    DRAM Throughput                     %         0.06
    Duration                      usecond         8.19
    L1/TEX Cache Throughput             %         0.91
    L2 Cache Throughput                 %         0.91
    SM Active Cycles                cycle      1055.93
    Compute (SM) Throughput             %         2.48
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 1, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.55
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle        11103
    Memory Throughput                   %         0.94
    DRAM Throughput                     %         0.10
    Duration                      usecond         8.32
    L1/TEX Cache Throughput             %         1.06
    L2 Cache Throughput                 %         0.94
    SM Active Cycles                cycle      1073.32
    Compute (SM) Throughput             %         2.49
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__reduce::ReduceAgent<thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>, thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>(T2, T3, T4, T5) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.82
    SM Frequency            cycle/nsecond         1.22
    Elapsed Cycles                  cycle         5103
    Memory Throughput                   %         2.80
    DRAM Throughput                     %         0.03
    Duration                      usecond         4.19
    L1/TEX Cache Throughput             %        18.64
    L2 Cache Throughput                 %         2.80
    SM Active Cycles                cycle        48.35
    Compute (SM) Throughput             %         0.15
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        13.95
    Achieved Active Warps Per SM           warp         6.69
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (13.9%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>, thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.27
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         3180
    Memory Throughput                   %         0.66
    DRAM Throughput                     %         0.02
    Duration                      usecond         2.46
    L1/TEX Cache Throughput             %        44.70
    L2 Cache Throughput                 %         0.66
    SM Active Cycles                cycle        20.13
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        12.02
    Achieved Active Warps Per SM           warp         5.77
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>, thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.40
    SM Frequency            cycle/nsecond         1.62
    Elapsed Cycles                  cycle         2690
    Memory Throughput                   %         0.58
    DRAM Throughput                     %            0
    Duration                      usecond         1.66
    L1/TEX Cache Throughput             %       222.55
    L2 Cache Throughput                 %         0.58
    SM Active Cycles                cycle         4.04
    Compute (SM) Throughput             %         0.00
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.08
    Achieved Active Warps Per SM           warp         7.72
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (16.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::updateUnknownsKernel(const double *, const double *, const double *, const double *, const double *, const double *, const double *, const double *, double *, double *, double *, double, double, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.30
    SM Frequency            cycle/nsecond         1.45
    Elapsed Cycles                  cycle         5948
    Memory Throughput                   %        12.06
    DRAM Throughput                     %        12.06
    Duration                      usecond         4.10
    L1/TEX Cache Throughput             %         7.39
    L2 Cache Throughput                 %         9.87
    SM Active Cycles                cycle      3159.10
    Compute (SM) Throughput             %         4.56
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         3.94
    Achieved Active Warps Per SM           warp         1.89
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (66.7%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (66.7%) and measured achieved occupancy (3.9%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.25
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         4010
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.10
    L1/TEX Cache Throughput             %        27.48
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        32.75
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.05
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.13
    SM Frequency            cycle/nsecond         1.27
    Elapsed Cycles                  cycle         3969
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.14
    L1/TEX Cache Throughput             %        28.35
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.75
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.10
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.34
    SM Frequency            cycle/nsecond         1.30
    Elapsed Cycles                  cycle         3922
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.03
    Duration                      usecond         3.01
    L1/TEX Cache Throughput             %        28.87
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.18
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelLeftBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.23
    SM Frequency            cycle/nsecond         1.28
    Elapsed Cycles                  cycle         4233
    Memory Throughput                   %         0.56
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.30
    L1/TEX Cache Throughput             %         3.40
    L2 Cache Throughput                 %         0.56
    SM Active Cycles                cycle       264.96
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.10
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelRightBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.12
    SM Frequency            cycle/nsecond         1.26
    Elapsed Cycles                  cycle         4293
    Memory Throughput                   %         0.55
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.39
    L1/TEX Cache Throughput             %         3.26
    L2 Cache Throughput                 %         0.55
    SM Active Cycles                cycle       275.69
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.05
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelBottomBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.52
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle         4419
    Memory Throughput                   %         0.64
    DRAM Throughput                     %         0.28
    Duration                      usecond         3.33
    L1/TEX Cache Throughput             %         3.34
    L2 Cache Throughput                 %         0.64
    SM Active Cycles                cycle       276.85
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.12
    Achieved Active Warps Per SM           warp         1.02
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelTopBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.24
    SM Frequency            cycle/nsecond         1.28
    Elapsed Cycles                  cycle         4357
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.29
    Duration                      usecond         3.39
    L1/TEX Cache Throughput             %         3.30
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle       279.94
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.06
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.98
    SM Frequency            cycle/nsecond         1.40
    Elapsed Cycles                  cycle        20062
    Memory Throughput                   %         2.63
    DRAM Throughput                     %         1.33
    Duration                      usecond        14.34
    L1/TEX Cache Throughput             %         2.66
    L2 Cache Throughput                 %         2.63
    SM Active Cycles                cycle     16068.91
    Compute (SM) Throughput             %        41.43
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           18
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.15
    Achieved Active Warps Per SM           warp         1.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (66.7%) and measured achieved occupancy (4.2%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (1, 8, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.67
    SM Frequency            cycle/nsecond         1.35
    Elapsed Cycles                  cycle        10961
    Memory Throughput                   %         0.91
    DRAM Throughput                     %         0.06
    Duration                      usecond         8.13
    L1/TEX Cache Throughput             %         0.91
    L2 Cache Throughput                 %         0.91
    SM Active Cycles                cycle      1056.72
    Compute (SM) Throughput             %         2.49
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 1, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.64
    SM Frequency            cycle/nsecond         1.35
    Elapsed Cycles                  cycle        11115
    Memory Throughput                   %         0.95
    DRAM Throughput                     %         0.11
    Duration                      usecond         8.26
    L1/TEX Cache Throughput             %         1.06
    L2 Cache Throughput                 %         0.95
    SM Active Cycles                cycle      1073.85
    Compute (SM) Throughput             %         2.49
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__reduce::ReduceAgent<thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>, thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>(T2, T3, T4, T5) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.94
    SM Frequency            cycle/nsecond         1.40
    Elapsed Cycles                  cycle         5856
    Memory Throughput                   %         2.44
    DRAM Throughput                     %         0.03
    Duration                      usecond         4.19
    L1/TEX Cache Throughput             %        18.60
    L2 Cache Throughput                 %         2.44
    SM Active Cycles                cycle        48.46
    Compute (SM) Throughput             %         0.13
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.01
    Achieved Active Warps Per SM           warp         6.72
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (14.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>, thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.21
    SM Frequency            cycle/nsecond         1.27
    Elapsed Cycles                  cycle         3172
    Memory Throughput                   %         0.66
    DRAM Throughput                     %         0.01
    Duration                      usecond         2.50
    L1/TEX Cache Throughput             %        45.07
    L2 Cache Throughput                 %         0.66
    SM Active Cycles                cycle        19.97
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        12.25
    Achieved Active Warps Per SM           warp         5.88
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.3%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>, thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.46
    SM Frequency            cycle/nsecond         1.61
    Elapsed Cycles                  cycle         2687
    Memory Throughput                   %         0.58
    DRAM Throughput                     %            0
    Duration                      usecond         1.66
    L1/TEX Cache Throughput             %       208.16
    L2 Cache Throughput                 %         0.58
    SM Active Cycles                cycle         4.32
    Compute (SM) Throughput             %         0.00
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.18
    Achieved Active Warps Per SM           warp         7.77
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (16.2%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::updateUnknownsKernel(const double *, const double *, const double *, const double *, const double *, const double *, const double *, const double *, double *, double *, double *, double, double, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.90
    SM Frequency            cycle/nsecond         1.38
    Elapsed Cycles                  cycle         5837
    Memory Throughput                   %        12.24
    DRAM Throughput                     %        12.24
    Duration                      usecond         4.22
    L1/TEX Cache Throughput             %         7.76
    L2 Cache Throughput                 %        10.06
    SM Active Cycles                cycle      3007.06
    Compute (SM) Throughput             %         4.65
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.50
    Achieved Active Warps Per SM           warp         2.16
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (66.7%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (66.7%) and measured achieved occupancy (4.5%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.29
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         4045
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.14
    L1/TEX Cache Throughput             %        27.56
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        32.66
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.10
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.35
    SM Frequency            cycle/nsecond         1.30
    Elapsed Cycles                  cycle         4047
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.02
    Duration                      usecond         3.10
    L1/TEX Cache Throughput             %        28.03
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        32.10
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.06
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.42
    SM Frequency            cycle/nsecond         1.31
    Elapsed Cycles                  cycle         3977
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.04
    L1/TEX Cache Throughput             %        26.36
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        34.15
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         1.91
    Achieved Active Warps Per SM           warp         0.92
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (1.9%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelLeftBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.70
    SM Frequency            cycle/nsecond         1.36
    Elapsed Cycles                  cycle         4357
    Memory Throughput                   %         0.54
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.20
    L1/TEX Cache Throughput             %         3.39
    L2 Cache Throughput                 %         0.54
    SM Active Cycles                cycle       265.38
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.13
    Achieved Active Warps Per SM           warp         1.02
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelRightBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.55
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle         4336
    Memory Throughput                   %         0.54
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.23
    L1/TEX Cache Throughput             %         3.29
    L2 Cache Throughput                 %         0.54
    SM Active Cycles                cycle       273.71
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.13
    Achieved Active Warps Per SM           warp         1.02
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelBottomBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.46
    SM Frequency            cycle/nsecond         1.32
    Elapsed Cycles                  cycle         4380
    Memory Throughput                   %         0.72
    DRAM Throughput                     %         0.28
    Duration                      usecond         3.33
    L1/TEX Cache Throughput             %         3.03
    L2 Cache Throughput                 %         0.72
    SM Active Cycles                cycle       305.49
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         1.91
    Achieved Active Warps Per SM           warp         0.92
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (1.9%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelTopBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.46
    SM Frequency            cycle/nsecond         1.32
    Elapsed Cycles                  cycle         4378
    Memory Throughput                   %         0.64
    DRAM Throughput                     %         0.27
    Duration                      usecond         3.33
    L1/TEX Cache Throughput             %         3.21
    L2 Cache Throughput                 %         0.64
    SM Active Cycles                cycle       288.35
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.01
    Achieved Active Warps Per SM           warp         0.96
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.0%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.93
    SM Frequency            cycle/nsecond         1.39
    Elapsed Cycles                  cycle        20033
    Memory Throughput                   %         2.63
    DRAM Throughput                     %         1.33
    Duration                      usecond        14.43
    L1/TEX Cache Throughput             %         2.66
    L2 Cache Throughput                 %         2.63
    SM Active Cycles                cycle     16085.22
    Compute (SM) Throughput             %        41.31
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           18
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.14
    Achieved Active Warps Per SM           warp         1.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (66.7%) and measured achieved occupancy (4.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (1, 8, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.58
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle        10919
    Memory Throughput                   %         0.92
    DRAM Throughput                     %         0.06
    Duration                      usecond         8.16
    L1/TEX Cache Throughput             %         0.89
    L2 Cache Throughput                 %         0.92
    SM Active Cycles                cycle      1077.24
    Compute (SM) Throughput             %         2.50
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.04
    Achieved Active Warps Per SM           warp         0.98
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.0%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 1, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.65
    SM Frequency            cycle/nsecond         1.35
    Elapsed Cycles                  cycle        11093
    Memory Throughput                   %         0.97
    DRAM Throughput                     %         0.11
    Duration                      usecond         8.22
    L1/TEX Cache Throughput             %         1.06
    L2 Cache Throughput                 %         0.97
    SM Active Cycles                cycle      1074.62
    Compute (SM) Throughput             %         2.49
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__reduce::ReduceAgent<thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>, thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>(T2, T3, T4, T5) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.60
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle         5635
    Memory Throughput                   %         2.54
    DRAM Throughput                     %         0.03
    Duration                      usecond         4.19
    L1/TEX Cache Throughput             %        18.93
    L2 Cache Throughput                 %         2.54
    SM Active Cycles                cycle        47.63
    Compute (SM) Throughput             %         0.13
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.27
    Achieved Active Warps Per SM           warp         6.85
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (14.3%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>, thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.34
    SM Frequency            cycle/nsecond         1.30
    Elapsed Cycles                  cycle         3283
    Memory Throughput                   %         0.64
    DRAM Throughput                     %         0.01
    Duration                      usecond         2.53
    L1/TEX Cache Throughput             %        45.07
    L2 Cache Throughput                 %         0.64
    SM Active Cycles                cycle        19.97
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        12.05
    Achieved Active Warps Per SM           warp         5.79
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>, thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.91
    SM Frequency            cycle/nsecond         1.71
    Elapsed Cycles                  cycle         2683
    Memory Throughput                   %         0.58
    DRAM Throughput                     %            0
    Duration                      usecond         1.57
    L1/TEX Cache Throughput             %       221.74
    L2 Cache Throughput                 %         0.58
    SM Active Cycles                cycle         4.06
    Compute (SM) Throughput             %         0.00
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.09
    Achieved Active Warps Per SM           warp         7.72
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (16.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::updateUnknownsKernel(const double *, const double *, const double *, const double *, const double *, const double *, const double *, const double *, double *, double *, double *, double, double, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.40
    SM Frequency            cycle/nsecond         1.46
    Elapsed Cycles                  cycle         5904
    Memory Throughput                   %        12.13
    DRAM Throughput                     %        12.13
    Duration                      usecond         4.03
    L1/TEX Cache Throughput             %         7.46
    L2 Cache Throughput                 %         9.94
    SM Active Cycles                cycle      3130.63
    Compute (SM) Throughput             %         4.59
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         3.96
    Achieved Active Warps Per SM           warp         1.90
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (66.7%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (66.7%) and measured achieved occupancy (4.0%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.49
    SM Frequency            cycle/nsecond         1.32
    Elapsed Cycles                  cycle         4012
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.04
    L1/TEX Cache Throughput             %        27.36
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        32.90
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.51
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle         3999
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.02
    Duration                      usecond         3.01
    L1/TEX Cache Throughput             %        28.20
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        31.91
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.10
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.31
    SM Frequency            cycle/nsecond         1.30
    Elapsed Cycles                  cycle         3907
    Memory Throughput                   %         0.48
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.01
    L1/TEX Cache Throughput             %        28.90
    L2 Cache Throughput                 %         0.48
    SM Active Cycles                cycle        31.15
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.19
    Achieved Active Warps Per SM           warp         1.05
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.2%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelLeftBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.58
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle         4256
    Memory Throughput                   %         0.56
    DRAM Throughput                     %         0.09
    Duration                      usecond         3.20
    L1/TEX Cache Throughput             %         3.16
    L2 Cache Throughput                 %         0.56
    SM Active Cycles                cycle       284.96
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         1.95
    Achieved Active Warps Per SM           warp         0.93
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (1.9%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelRightBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.38
    SM Frequency            cycle/nsecond         1.30
    Elapsed Cycles                  cycle         4253
    Memory Throughput                   %         0.55
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.26
    L1/TEX Cache Throughput             %         3.28
    L2 Cache Throughput                 %         0.55
    SM Active Cycles                cycle       274.46
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.05
    Achieved Active Warps Per SM           warp         0.98
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelBottomBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.23
    SM Frequency            cycle/nsecond         1.28
    Elapsed Cycles                  cycle         4314
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.30
    Duration                      usecond         3.36
    L1/TEX Cache Throughput             %         3.30
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle       279.91
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelTopBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.89
    SM Frequency            cycle/nsecond         1.39
    Elapsed Cycles                  cycle         4570
    Memory Throughput                   %         0.62
    DRAM Throughput                     %         0.27
    Duration                      usecond         3.30
    L1/TEX Cache Throughput             %         3.35
    L2 Cache Throughput                 %         0.62
    SM Active Cycles                cycle       276.41
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.95
    SM Frequency            cycle/nsecond         1.39
    Elapsed Cycles                  cycle        20067
    Memory Throughput                   %         2.63
    DRAM Throughput                     %         1.33
    Duration                      usecond        14.40
    L1/TEX Cache Throughput             %         2.66
    L2 Cache Throughput                 %         2.63
    SM Active Cycles                cycle     16045.22
    Compute (SM) Throughput             %        41.16
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           18
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.15
    Achieved Active Warps Per SM           warp         1.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (66.7%) and measured achieved occupancy (4.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (1, 8, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.59
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle        10957
    Memory Throughput                   %         0.91
    DRAM Throughput                     %         0.06
    Duration                      usecond         8.19
    L1/TEX Cache Throughput             %         0.91
    L2 Cache Throughput                 %         0.91
    SM Active Cycles                cycle      1056.75
    Compute (SM) Throughput             %         2.49
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 1, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.57
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle        11122
    Memory Throughput                   %         0.94
    DRAM Throughput                     %         0.11
    Duration                      usecond         8.32
    L1/TEX Cache Throughput             %         1.06
    L2 Cache Throughput                 %         0.94
    SM Active Cycles                cycle      1074.81
    Compute (SM) Throughput             %         2.49
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__reduce::ReduceAgent<thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>, thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>(T2, T3, T4, T5) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.96
    SM Frequency            cycle/nsecond         1.39
    Elapsed Cycles                  cycle         5848
    Memory Throughput                   %         2.45
    DRAM Throughput                     %         0.03
    Duration                      usecond         4.19
    L1/TEX Cache Throughput             %        18.44
    L2 Cache Throughput                 %         2.45
    SM Active Cycles                cycle        48.88
    Compute (SM) Throughput             %         0.13
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.07
    Achieved Active Warps Per SM           warp         6.75
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (14.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>, thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.98
    SM Frequency            cycle/nsecond         1.25
    Elapsed Cycles                  cycle         3150
    Memory Throughput                   %         0.67
    DRAM Throughput                     %         0.01
    Duration                      usecond         2.53
    L1/TEX Cache Throughput             %        44.28
    L2 Cache Throughput                 %         0.67
    SM Active Cycles                cycle        20.32
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        11.78
    Achieved Active Warps Per SM           warp         5.65
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (11.8%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>, thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.90
    SM Frequency            cycle/nsecond         1.54
    Elapsed Cycles                  cycle         2668
    Memory Throughput                   %         0.59
    DRAM Throughput                     %            0
    Duration                      usecond         1.73
    L1/TEX Cache Throughput             %       207.46
    L2 Cache Throughput                 %         0.59
    SM Active Cycles                cycle         4.34
    Compute (SM) Throughput             %         0.00
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.07
    Achieved Active Warps Per SM           warp         7.71
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (16.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::updateUnknownsKernel(const double *, const double *, const double *, const double *, const double *, const double *, const double *, const double *, double *, double *, double *, double, double, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond            9
    SM Frequency            cycle/nsecond         1.40
    Elapsed Cycles                  cycle         5745
    Memory Throughput                   %        12.47
    DRAM Throughput                     %        12.47
    Duration                      usecond         4.10
    L1/TEX Cache Throughput             %         7.38
    L2 Cache Throughput                 %        10.24
    SM Active Cycles                cycle      3163.38
    Compute (SM) Throughput             %         4.72
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.04
    Achieved Active Warps Per SM           warp         1.94
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (66.7%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (66.7%) and measured achieved occupancy (4.0%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.15
    SM Frequency            cycle/nsecond         1.27
    Elapsed Cycles                  cycle         4026
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.02
    Duration                      usecond         3.17
    L1/TEX Cache Throughput             %        27.82
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        32.35
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.12
    Achieved Active Warps Per SM           warp         1.02
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.21
    SM Frequency            cycle/nsecond         1.28
    Elapsed Cycles                  cycle         3979
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.10
    L1/TEX Cache Throughput             %        28.40
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.69
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.23
    SM Frequency            cycle/nsecond         1.28
    Elapsed Cycles                  cycle         3938
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.02
    Duration                      usecond         3.07
    L1/TEX Cache Throughput             %        28.58
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.49
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelLeftBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.42
    SM Frequency            cycle/nsecond         1.32
    Elapsed Cycles                  cycle         4340
    Memory Throughput                   %         0.55
    DRAM Throughput                     %         0.09
    Duration                      usecond         3.30
    L1/TEX Cache Throughput             %         3.37
    L2 Cache Throughput                 %         0.55
    SM Active Cycles                cycle       267.26
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelRightBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.31
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         4309
    Memory Throughput                   %         0.54
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.33
    L1/TEX Cache Throughput             %         3.20
    L2 Cache Throughput                 %         0.54
    SM Active Cycles                cycle       281.54
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.03
    Achieved Active Warps Per SM           warp         0.97
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.0%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelBottomBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.31
    SM Frequency            cycle/nsecond         1.30
    Elapsed Cycles                  cycle         4332
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.28
    Duration                      usecond         3.33
    L1/TEX Cache Throughput             %         3.31
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle       279.71
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelTopBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.47
    SM Frequency            cycle/nsecond         1.32
    Elapsed Cycles                  cycle         4430
    Memory Throughput                   %         0.63
    DRAM Throughput                     %         0.28
    Duration                      usecond         3.36
    L1/TEX Cache Throughput             %         3.24
    L2 Cache Throughput                 %         0.63
    SM Active Cycles                cycle       285.88
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.04
    Achieved Active Warps Per SM           warp         0.98
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.0%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.99
    SM Frequency            cycle/nsecond         1.40
    Elapsed Cycles                  cycle        20078
    Memory Throughput                   %         2.63
    DRAM Throughput                     %         1.33
    Duration                      usecond        14.34
    L1/TEX Cache Throughput             %         2.66
    L2 Cache Throughput                 %         2.63
    SM Active Cycles                cycle     15809.69
    Compute (SM) Throughput             %        40.72
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           18
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.15
    Achieved Active Warps Per SM           warp         1.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (66.7%) and measured achieved occupancy (4.2%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (1, 8, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.61
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle        11012
    Memory Throughput                   %         0.91
    DRAM Throughput                     %         0.06
    Duration                      usecond         8.19
    L1/TEX Cache Throughput             %         0.91
    L2 Cache Throughput                 %         0.91
    SM Active Cycles                cycle      1054.49
    Compute (SM) Throughput             %         2.48
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 1, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.62
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle        11086
    Memory Throughput                   %         0.94
    DRAM Throughput                     %         0.11
    Duration                      usecond         8.26
    L1/TEX Cache Throughput             %         1.06
    L2 Cache Throughput                 %         0.94
    SM Active Cycles                cycle      1074.34
    Compute (SM) Throughput             %         2.50
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__reduce::ReduceAgent<thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>, thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>(T2, T3, T4, T5) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.68
    SM Frequency            cycle/nsecond         1.20
    Elapsed Cycles                  cycle         5078
    Memory Throughput                   %         2.82
    DRAM Throughput                     %         0.03
    Duration                      usecond         4.22
    L1/TEX Cache Throughput             %        18.67
    L2 Cache Throughput                 %         2.82
    SM Active Cycles                cycle        48.29
    Compute (SM) Throughput             %         0.15
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.21
    Achieved Active Warps Per SM           warp         6.82
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (14.2%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>, thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.19
    SM Frequency            cycle/nsecond         1.28
    Elapsed Cycles                  cycle         3164
    Memory Throughput                   %         0.66
    DRAM Throughput                     %         0.02
    Duration                      usecond         2.46
    L1/TEX Cache Throughput             %        45.10
    L2 Cache Throughput                 %         0.66
    SM Active Cycles                cycle        19.96
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        12.15
    Achieved Active Warps Per SM           warp         5.83
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>, thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.08
    SM Frequency            cycle/nsecond         1.59
    Elapsed Cycles                  cycle         2694
    Memory Throughput                   %         0.58
    DRAM Throughput                     %            0
    Duration                      usecond         1.70
    L1/TEX Cache Throughput             %       221.74
    L2 Cache Throughput                 %         0.58
    SM Active Cycles                cycle         4.06
    Compute (SM) Throughput             %         0.00
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        15.96
    Achieved Active Warps Per SM           warp         7.66
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (16.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::updateUnknownsKernel(const double *, const double *, const double *, const double *, const double *, const double *, const double *, const double *, double *, double *, double *, double, double, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.35
    SM Frequency            cycle/nsecond         1.45
    Elapsed Cycles                  cycle         6045
    Memory Throughput                   %        11.82
    DRAM Throughput                     %        11.82
    Duration                      usecond         4.16
    L1/TEX Cache Throughput             %         7.53
    L2 Cache Throughput                 %         9.72
    SM Active Cycles                cycle      3101.51
    Compute (SM) Throughput             %         4.49
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.09
    Achieved Active Warps Per SM           warp         1.96
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (66.7%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (66.7%) and measured achieved occupancy (4.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.33
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         4061
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.14
    L1/TEX Cache Throughput             %        27.33
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        32.93
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.18
    Achieved Active Warps Per SM           warp         1.05
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.2%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.13
    SM Frequency            cycle/nsecond         1.27
    Elapsed Cycles                  cycle         3979
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.14
    L1/TEX Cache Throughput             %        27.89
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        32.26
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.12
    SM Frequency            cycle/nsecond         1.27
    Elapsed Cycles                  cycle         3938
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.10
    L1/TEX Cache Throughput             %        28.68
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.38
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.15
    Achieved Active Warps Per SM           warp         1.03
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelLeftBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.52
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle         4301
    Memory Throughput                   %         0.55
    DRAM Throughput                     %         0.09
    Duration                      usecond         3.23
    L1/TEX Cache Throughput             %         3.38
    L2 Cache Throughput                 %         0.55
    SM Active Cycles                cycle       266.60
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.10
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelRightBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.54
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle         4376
    Memory Throughput                   %         0.54
    DRAM Throughput                     %         0.07
    Duration                      usecond         3.30
    L1/TEX Cache Throughput             %         3.19
    L2 Cache Throughput                 %         0.54
    SM Active Cycles                cycle       282.03
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.05
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelBottomBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.45
    SM Frequency            cycle/nsecond         1.32
    Elapsed Cycles                  cycle         4355
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.28
    Duration                      usecond         3.30
    L1/TEX Cache Throughput             %         3.34
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle       276.74
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelTopBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.23
    SM Frequency            cycle/nsecond         1.28
    Elapsed Cycles                  cycle         4300
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.29
    Duration                      usecond         3.36
    L1/TEX Cache Throughput             %         3.30
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle       280.04
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.88
    SM Frequency            cycle/nsecond         1.38
    Elapsed Cycles                  cycle        20050
    Memory Throughput                   %         2.63
    DRAM Throughput                     %         1.33
    Duration                      usecond        14.50
    L1/TEX Cache Throughput             %         2.66
    L2 Cache Throughput                 %         2.63
    SM Active Cycles                cycle     15546.76
    Compute (SM) Throughput             %        40.32
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           18
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.14
    Achieved Active Warps Per SM           warp         1.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (66.7%) and measured achieved occupancy (4.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (1, 8, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.53
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle        10941
    Memory Throughput                   %         0.91
    DRAM Throughput                     %         0.06
    Duration                      usecond         8.22
    L1/TEX Cache Throughput             %         0.91
    L2 Cache Throughput                 %         0.91
    SM Active Cycles                cycle      1055.62
    Compute (SM) Throughput             %         2.49
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.10
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 1, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.69
    SM Frequency            cycle/nsecond         1.36
    Elapsed Cycles                  cycle        11167
    Memory Throughput                   %         0.94
    DRAM Throughput                     %         0.11
    Duration                      usecond         8.22
    L1/TEX Cache Throughput             %         1.05
    L2 Cache Throughput                 %         0.94
    SM Active Cycles                cycle      1078.43
    Compute (SM) Throughput             %         2.48
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__reduce::ReduceAgent<thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>, thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>(T2, T3, T4, T5) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.90
    SM Frequency            cycle/nsecond         1.39
    Elapsed Cycles                  cycle         5864
    Memory Throughput                   %         2.44
    DRAM Throughput                     %         0.03
    Duration                      usecond         4.22
    L1/TEX Cache Throughput             %        18.55
    L2 Cache Throughput                 %         2.44
    SM Active Cycles                cycle        48.59
    Compute (SM) Throughput             %         0.13
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.20
    Achieved Active Warps Per SM           warp         6.81
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (14.2%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>, thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.44
    SM Frequency            cycle/nsecond         1.31
    Elapsed Cycles                  cycle         3235
    Memory Throughput                   %         0.73
    DRAM Throughput                     %         0.02
    Duration                      usecond         2.46
    L1/TEX Cache Throughput             %        45.10
    L2 Cache Throughput                 %         0.73
    SM Active Cycles                cycle        19.96
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        11.98
    Achieved Active Warps Per SM           warp         5.75
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>, thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        11.17
    SM Frequency            cycle/nsecond         1.73
    Elapsed Cycles                  cycle         2939
    Memory Throughput                   %         0.53
    DRAM Throughput                     %            0
    Duration                      usecond         1.70
    L1/TEX Cache Throughput             %       208.16
    L2 Cache Throughput                 %         0.53
    SM Active Cycles                cycle         4.32
    Compute (SM) Throughput             %         0.00
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.18
    Achieved Active Warps Per SM           warp         7.77
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (16.2%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::updateUnknownsKernel(const double *, const double *, const double *, const double *, const double *, const double *, const double *, const double *, double *, double *, double *, double, double, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.97
    SM Frequency            cycle/nsecond         1.39
    Elapsed Cycles                  cycle         5939
    Memory Throughput                   %        12.04
    DRAM Throughput                     %        12.04
    Duration                      usecond         4.26
    L1/TEX Cache Throughput             %         7.75
    L2 Cache Throughput                 %         9.89
    SM Active Cycles                cycle      3011.93
    Compute (SM) Throughput             %         4.57
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.14
    Achieved Active Warps Per SM           warp         1.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (66.7%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (66.7%) and measured achieved occupancy (4.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.38
    SM Frequency            cycle/nsecond         1.30
    Elapsed Cycles                  cycle         4039
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.02
    Duration                      usecond         3.10
    L1/TEX Cache Throughput             %        27.35
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        32.91
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.06
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.33
    SM Frequency            cycle/nsecond         1.30
    Elapsed Cycles                  cycle         3991
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.07
    L1/TEX Cache Throughput             %        28.02
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        32.12
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.06
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.35
    SM Frequency            cycle/nsecond         1.30
    Elapsed Cycles                  cycle         3955
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.02
    Duration                      usecond         3.04
    L1/TEX Cache Throughput             %        28.61
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.46
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.10
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelLeftBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.78
    SM Frequency            cycle/nsecond         1.37
    Elapsed Cycles                  cycle         4440
    Memory Throughput                   %         0.53
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.23
    L1/TEX Cache Throughput             %         3.28
    L2 Cache Throughput                 %         0.53
    SM Active Cycles                cycle       274.46
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.04
    Achieved Active Warps Per SM           warp         0.98
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.0%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelRightBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.70
    SM Frequency            cycle/nsecond         1.36
    Elapsed Cycles                  cycle         4478
    Memory Throughput                   %         0.52
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.30
    L1/TEX Cache Throughput             %         3.28
    L2 Cache Throughput                 %         0.52
    SM Active Cycles                cycle       274.01
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelBottomBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.37
    SM Frequency            cycle/nsecond         1.31
    Elapsed Cycles                  cycle         4347
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.28
    Duration                      usecond         3.33
    L1/TEX Cache Throughput             %         3.31
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle       279.74
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelTopBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.48
    SM Frequency            cycle/nsecond         1.32
    Elapsed Cycles                  cycle         4346
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.28
    Duration                      usecond         3.30
    L1/TEX Cache Throughput             %         3.30
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle       280.43
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.95
    SM Frequency            cycle/nsecond         1.39
    Elapsed Cycles                  cycle        20062
    Memory Throughput                   %         2.63
    DRAM Throughput                     %         1.33
    Duration                      usecond        14.40
    L1/TEX Cache Throughput             %         2.66
    L2 Cache Throughput                 %         2.63
    SM Active Cycles                cycle     15409.49
    Compute (SM) Throughput             %        40.14
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           18
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.15
    Achieved Active Warps Per SM           warp         1.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (66.7%) and measured achieved occupancy (4.2%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (1, 8, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.75
    SM Frequency            cycle/nsecond         1.36
    Elapsed Cycles                  cycle        11104
    Memory Throughput                   %         0.90
    DRAM Throughput                     %         0.06
    Duration                      usecond         8.16
    L1/TEX Cache Throughput             %         0.91
    L2 Cache Throughput                 %         0.90
    SM Active Cycles                cycle      1056.04
    Compute (SM) Throughput             %         2.46
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 1, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.63
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle        11100
    Memory Throughput                   %         0.97
    DRAM Throughput                     %         0.11
    Duration                      usecond         8.26
    L1/TEX Cache Throughput             %         1.06
    L2 Cache Throughput                 %         0.97
    SM Active Cycles                cycle      1074.51
    Compute (SM) Throughput             %         2.49
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__reduce::ReduceAgent<thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>, thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>(T2, T3, T4, T5) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.63
    SM Frequency            cycle/nsecond         1.35
    Elapsed Cycles                  cycle         5685
    Memory Throughput                   %         2.52
    DRAM Throughput                     %         0.03
    Duration                      usecond         4.22
    L1/TEX Cache Throughput             %        18.53
    L2 Cache Throughput                 %         2.52
    SM Active Cycles                cycle        48.65
    Compute (SM) Throughput             %         0.13
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.11
    Achieved Active Warps Per SM           warp         6.77
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (14.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>, thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.10
    SM Frequency            cycle/nsecond         1.26
    Elapsed Cycles                  cycle         3182
    Memory Throughput                   %         0.66
    DRAM Throughput                     %         0.01
    Duration                      usecond         2.53
    L1/TEX Cache Throughput             %        45.07
    L2 Cache Throughput                 %         0.66
    SM Active Cycles                cycle        19.97
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        11.96
    Achieved Active Warps Per SM           warp         5.74
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>, thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.66
    SM Frequency            cycle/nsecond         1.51
    Elapsed Cycles                  cycle         2511
    Memory Throughput                   %         0.62
    DRAM Throughput                     %            0
    Duration                      usecond         1.66
    L1/TEX Cache Throughput             %       223.36
    L2 Cache Throughput                 %         0.62
    SM Active Cycles                cycle         4.03
    Compute (SM) Throughput             %         0.00
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.20
    Achieved Active Warps Per SM           warp         7.78
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (16.2%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::updateUnknownsKernel(const double *, const double *, const double *, const double *, const double *, const double *, const double *, const double *, double *, double *, double *, double, double, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.85
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle         6175
    Memory Throughput                   %        11.58
    DRAM Throughput                     %        11.58
    Duration                      usecond         4.03
    L1/TEX Cache Throughput             %         7.73
    L2 Cache Throughput                 %         9.51
    SM Active Cycles                cycle      3021.41
    Compute (SM) Throughput             %         4.40
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.12
    Achieved Active Warps Per SM           warp         1.98
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (66.7%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (66.7%) and measured achieved occupancy (4.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.45
    SM Frequency            cycle/nsecond         1.31
    Elapsed Cycles                  cycle         4078
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.02
    Duration                      usecond         3.10
    L1/TEX Cache Throughput             %        26.56
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        33.88
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.03
    Achieved Active Warps Per SM           warp         0.97
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.0%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.23
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         3972
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.03
    Duration                      usecond         3.07
    L1/TEX Cache Throughput             %        28.18
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.94
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.06
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.56
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle         3938
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.01
    Duration                      usecond         2.94
    L1/TEX Cache Throughput             %        28.65
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.41
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelLeftBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.58
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle         4261
    Memory Throughput                   %         0.56
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.20
    L1/TEX Cache Throughput             %         3.40
    L2 Cache Throughput                 %         0.56
    SM Active Cycles                cycle       264.96
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelRightBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.54
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle         4290
    Memory Throughput                   %         0.55
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.20
    L1/TEX Cache Throughput             %         3.23
    L2 Cache Throughput                 %         0.55
    SM Active Cycles                cycle       278.41
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelBottomBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.56
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle         4343
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.29
    Duration                      usecond         3.26
    L1/TEX Cache Throughput             %         3.31
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle       279.26
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelTopBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.75
    SM Frequency            cycle/nsecond         1.37
    Elapsed Cycles                  cycle         4457
    Memory Throughput                   %         0.63
    DRAM Throughput                     %         0.28
    Duration                      usecond         3.26
    L1/TEX Cache Throughput             %         3.31
    L2 Cache Throughput                 %         0.63
    SM Active Cycles                cycle       279.28
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.88
    SM Frequency            cycle/nsecond         1.38
    Elapsed Cycles                  cycle        20064
    Memory Throughput                   %         2.63
    DRAM Throughput                     %         1.33
    Duration                      usecond        14.50
    L1/TEX Cache Throughput             %         2.66
    L2 Cache Throughput                 %         2.63
    SM Active Cycles                cycle     15426.37
    Compute (SM) Throughput             %        40.13
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           18
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (66.7%) and measured achieved occupancy (4.2%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (1, 8, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.53
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle        10929
    Memory Throughput                   %         0.92
    DRAM Throughput                     %         0.06
    Duration                      usecond         8.22
    L1/TEX Cache Throughput             %         0.91
    L2 Cache Throughput                 %         0.92
    SM Active Cycles                cycle      1059.40
    Compute (SM) Throughput             %         2.50
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 1, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.59
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle        11106
    Memory Throughput                   %         0.94
    DRAM Throughput                     %         0.11
    Duration                      usecond         8.32
    L1/TEX Cache Throughput             %         1.06
    L2 Cache Throughput                 %         0.94
    SM Active Cycles                cycle      1074.91
    Compute (SM) Throughput             %         2.49
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__reduce::ReduceAgent<thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>, thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>(T2, T3, T4, T5) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.23
    SM Frequency            cycle/nsecond         1.44
    Elapsed Cycles                  cycle         5979
    Memory Throughput                   %         2.39
    DRAM Throughput                     %         0.03
    Duration                      usecond         4.16
    L1/TEX Cache Throughput             %        18.68
    L2 Cache Throughput                 %         2.39
    SM Active Cycles                cycle        48.25
    Compute (SM) Throughput             %         0.13
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.19
    Achieved Active Warps Per SM           warp         6.81
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (14.2%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>, thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.21
    SM Frequency            cycle/nsecond         1.28
    Elapsed Cycles                  cycle         3208
    Memory Throughput                   %         0.72
    DRAM Throughput                     %         0.01
    Duration                      usecond         2.50
    L1/TEX Cache Throughput             %        44.35
    L2 Cache Throughput                 %         0.72
    SM Active Cycles                cycle        20.29
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        11.75
    Achieved Active Warps Per SM           warp         5.64
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (11.7%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>, thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.48
    SM Frequency            cycle/nsecond         1.49
    Elapsed Cycles                  cycle         2527
    Memory Throughput                   %         0.62
    DRAM Throughput                     %            0
    Duration                      usecond         1.70
    L1/TEX Cache Throughput             %       207.46
    L2 Cache Throughput                 %         0.62
    SM Active Cycles                cycle         4.34
    Compute (SM) Throughput             %         0.00
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.07
    Achieved Active Warps Per SM           warp         7.71
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (16.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::updateUnknownsKernel(const double *, const double *, const double *, const double *, const double *, const double *, const double *, const double *, double *, double *, double *, double, double, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.16
    SM Frequency            cycle/nsecond         1.43
    Elapsed Cycles                  cycle         5734
    Memory Throughput                   %        12.55
    DRAM Throughput                     %        12.55
    Duration                      usecond            4
    L1/TEX Cache Throughput             %         7.72
    L2 Cache Throughput                 %        10.24
    SM Active Cycles                cycle      3024.66
    Compute (SM) Throughput             %         4.73
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.36
    Achieved Active Warps Per SM           warp         2.09
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (66.7%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (66.7%) and measured achieved occupancy (4.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.23
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         4041
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.02
    Duration                      usecond         3.14
    L1/TEX Cache Throughput             %        27.25
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        33.03
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.25
    SM Frequency            cycle/nsecond         1.28
    Elapsed Cycles                  cycle         3971
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.02
    Duration                      usecond         3.10
    L1/TEX Cache Throughput             %        28.22
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.90
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.47
    SM Frequency            cycle/nsecond         1.32
    Elapsed Cycles                  cycle         4044
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.02
    Duration                      usecond         3.07
    L1/TEX Cache Throughput             %        28.87
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        31.18
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.12
    Achieved Active Warps Per SM           warp         1.02
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelLeftBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.41
    SM Frequency            cycle/nsecond         1.31
    Elapsed Cycles                  cycle         4287
    Memory Throughput                   %         0.55
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.26
    L1/TEX Cache Throughput             %         3.31
    L2 Cache Throughput                 %         0.55
    SM Active Cycles                cycle       271.76
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.04
    Achieved Active Warps Per SM           warp         0.98
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.0%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelRightBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.21
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         4364
    Memory Throughput                   %         0.54
    DRAM Throughput                     %         0.09
    Duration                      usecond         3.39
    L1/TEX Cache Throughput             %         3.28
    L2 Cache Throughput                 %         0.54
    SM Active Cycles                cycle       274.78
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.10
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelBottomBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.27
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         4365
    Memory Throughput                   %         0.64
    DRAM Throughput                     %         0.27
    Duration                      usecond         3.39
    L1/TEX Cache Throughput             %         3.31
    L2 Cache Throughput                 %         0.64
    SM Active Cycles                cycle       279.06
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.17
    Achieved Active Warps Per SM           warp         1.04
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.2%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelTopBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.47
    SM Frequency            cycle/nsecond         1.32
    Elapsed Cycles                  cycle         4421
    Memory Throughput                   %         0.64
    DRAM Throughput                     %         0.28
    Duration                      usecond         3.36
    L1/TEX Cache Throughput             %         3.32
    L2 Cache Throughput                 %         0.64
    SM Active Cycles                cycle       278.26
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.12
    Achieved Active Warps Per SM           warp         1.02
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.93
    SM Frequency            cycle/nsecond         1.39
    Elapsed Cycles                  cycle        19983
    Memory Throughput                   %         2.64
    DRAM Throughput                     %         1.34
    Duration                      usecond        14.37
    L1/TEX Cache Throughput             %         2.67
    L2 Cache Throughput                 %         2.64
    SM Active Cycles                cycle     15371.21
    Compute (SM) Throughput             %        39.99
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           18
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         1.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (66.7%) and measured achieved occupancy (4.2%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (1, 8, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.60
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle        10921
    Memory Throughput                   %         0.92
    DRAM Throughput                     %         0.05
    Duration                      usecond         8.16
    L1/TEX Cache Throughput             %         0.89
    L2 Cache Throughput                 %         0.92
    SM Active Cycles                cycle      1071.40
    Compute (SM) Throughput             %         2.50
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.05
    Achieved Active Warps Per SM           warp         0.98
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.0%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 1, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.62
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle        11145
    Memory Throughput                   %         0.94
    DRAM Throughput                     %         0.11
    Duration                      usecond         8.29
    L1/TEX Cache Throughput             %         1.06
    L2 Cache Throughput                 %         0.94
    SM Active Cycles                cycle      1073.79
    Compute (SM) Throughput             %         2.48
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__reduce::ReduceAgent<thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>, thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>(T2, T3, T4, T5) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.65
    SM Frequency            cycle/nsecond         1.35
    Elapsed Cycles                  cycle         5642
    Memory Throughput                   %         2.54
    DRAM Throughput                     %         0.03
    Duration                      usecond         4.19
    L1/TEX Cache Throughput             %        18.77
    L2 Cache Throughput                 %         2.54
    SM Active Cycles                cycle        48.03
    Compute (SM) Throughput             %         0.13
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.23
    Achieved Active Warps Per SM           warp         6.83
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (14.2%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>, thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.23
    SM Frequency            cycle/nsecond         1.28
    Elapsed Cycles                  cycle         3162
    Memory Throughput                   %         0.66
    DRAM Throughput                     %         0.01
    Duration                      usecond         2.46
    L1/TEX Cache Throughput             %        44.28
    L2 Cache Throughput                 %         0.66
    SM Active Cycles                cycle        20.32
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        11.89
    Achieved Active Warps Per SM           warp         5.71
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (11.9%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>, thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.36
    SM Frequency            cycle/nsecond         1.46
    Elapsed Cycles                  cycle         2518
    Memory Throughput                   %         0.62
    DRAM Throughput                     %            0
    Duration                      usecond         1.73
    L1/TEX Cache Throughput             %       222.55
    L2 Cache Throughput                 %         0.62
    SM Active Cycles                cycle         4.04
    Compute (SM) Throughput             %         0.00
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.08
    Achieved Active Warps Per SM           warp         7.72
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (16.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::updateUnknownsKernel(const double *, const double *, const double *, const double *, const double *, const double *, const double *, const double *, double *, double *, double *, double, double, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.83
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle         5485
    Memory Throughput                   %        13.01
    DRAM Throughput                     %        13.01
    Duration                      usecond         4.51
    L1/TEX Cache Throughput             %         7.71
    L2 Cache Throughput                 %        10.73
    SM Active Cycles                cycle      3028.01
    Compute (SM) Throughput             %         4.95
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.08
    Achieved Active Warps Per SM           warp         1.96
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (66.7%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (66.7%) and measured achieved occupancy (4.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.29
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         4036
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.02
    Duration                      usecond         3.14
    L1/TEX Cache Throughput             %        27.59
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        32.62
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.25
    SM Frequency            cycle/nsecond         1.28
    Elapsed Cycles                  cycle         3984
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.03
    Duration                      usecond         3.10
    L1/TEX Cache Throughput             %        28.07
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        32.06
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.10
    SM Frequency            cycle/nsecond         1.25
    Elapsed Cycles                  cycle         3927
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.14
    L1/TEX Cache Throughput             %        28.73
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.32
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelLeftBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.46
    SM Frequency            cycle/nsecond         1.31
    Elapsed Cycles                  cycle         4251
    Memory Throughput                   %         0.56
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.23
    L1/TEX Cache Throughput             %         3.34
    L2 Cache Throughput                 %         0.56
    SM Active Cycles                cycle       269.82
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.04
    Achieved Active Warps Per SM           warp         0.98
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.0%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelRightBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.54
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle         4278
    Memory Throughput                   %         0.55
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.20
    L1/TEX Cache Throughput             %         3.35
    L2 Cache Throughput                 %         0.55
    SM Active Cycles                cycle       268.59
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.15
    Achieved Active Warps Per SM           warp         1.03
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelBottomBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.52
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle         4421
    Memory Throughput                   %         0.64
    DRAM Throughput                     %         0.27
    Duration                      usecond         3.33
    L1/TEX Cache Throughput             %         3.32
    L2 Cache Throughput                 %         0.64
    SM Active Cycles                cycle       278.68
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelTopBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.37
    SM Frequency            cycle/nsecond         1.30
    Elapsed Cycles                  cycle         4328
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.29
    Duration                      usecond         3.33
    L1/TEX Cache Throughput             %         3.32
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle       278.75
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.06
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.96
    SM Frequency            cycle/nsecond         1.40
    Elapsed Cycles                  cycle        20075
    Memory Throughput                   %         2.63
    DRAM Throughput                     %         1.33
    Duration                      usecond        14.37
    L1/TEX Cache Throughput             %         2.66
    L2 Cache Throughput                 %         2.63
    SM Active Cycles                cycle     15326.90
    Compute (SM) Throughput             %        39.29
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           18
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.15
    Achieved Active Warps Per SM           warp         1.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (66.7%) and measured achieved occupancy (4.2%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (1, 8, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.67
    SM Frequency            cycle/nsecond         1.35
    Elapsed Cycles                  cycle        11158
    Memory Throughput                   %         0.93
    DRAM Throughput                     %         0.05
    Duration                      usecond         8.26
    L1/TEX Cache Throughput             %         0.91
    L2 Cache Throughput                 %         0.93
    SM Active Cycles                cycle      1057.49
    Compute (SM) Throughput             %         2.45
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 1, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.62
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle        11172
    Memory Throughput                   %         0.94
    DRAM Throughput                     %         0.11
    Duration                      usecond         8.32
    L1/TEX Cache Throughput             %         1.05
    L2 Cache Throughput                 %         0.94
    SM Active Cycles                cycle      1079.72
    Compute (SM) Throughput             %         2.48
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__reduce::ReduceAgent<thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>, thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>(T2, T3, T4, T5) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.14
    SM Frequency            cycle/nsecond         1.43
    Elapsed Cycles                  cycle         5982
    Memory Throughput                   %         2.39
    DRAM Throughput                     %         0.03
    Duration                      usecond         4.19
    L1/TEX Cache Throughput             %        18.65
    L2 Cache Throughput                 %         2.39
    SM Active Cycles                cycle        48.34
    Compute (SM) Throughput             %         0.13
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.10
    Achieved Active Warps Per SM           warp         6.77
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (14.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>, thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.21
    SM Frequency            cycle/nsecond         1.27
    Elapsed Cycles                  cycle         3175
    Memory Throughput                   %         0.66
    DRAM Throughput                     %         0.01
    Duration                      usecond         2.50
    L1/TEX Cache Throughput             %        44.80
    L2 Cache Throughput                 %         0.66
    SM Active Cycles                cycle        20.09
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        11.89
    Achieved Active Warps Per SM           warp         5.71
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (11.9%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>, thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.57
    SM Frequency            cycle/nsecond         1.64
    Elapsed Cycles                  cycle         2774
    Memory Throughput                   %         0.56
    DRAM Throughput                     %            0
    Duration                      usecond         1.70
    L1/TEX Cache Throughput             %       207.46
    L2 Cache Throughput                 %         0.56
    SM Active Cycles                cycle         4.34
    Compute (SM) Throughput             %         0.00
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.12
    Achieved Active Warps Per SM           warp         7.74
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (16.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::updateUnknownsKernel(const double *, const double *, const double *, const double *, const double *, const double *, const double *, const double *, double *, double *, double *, double, double, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.63
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle         5410
    Memory Throughput                   %        13.20
    DRAM Throughput                     %        13.20
    Duration                      usecond         4.03
    L1/TEX Cache Throughput             %         7.41
    L2 Cache Throughput                 %        10.85
    SM Active Cycles                cycle      3150.75
    Compute (SM) Throughput             %         5.02
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         3.94
    Achieved Active Warps Per SM           warp         1.89
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (66.7%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (66.7%) and measured achieved occupancy (3.9%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.16
    SM Frequency            cycle/nsecond         1.28
    Elapsed Cycles                  cycle         4008
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.14
    L1/TEX Cache Throughput             %        27.38
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        32.87
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp            1
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.16
    SM Frequency            cycle/nsecond         1.27
    Elapsed Cycles                  cycle         3978
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.02
    Duration                      usecond         3.14
    L1/TEX Cache Throughput             %        28.32
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.78
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.31
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         4009
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.10
    L1/TEX Cache Throughput             %        26.81
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        33.57
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         1.98
    Achieved Active Warps Per SM           warp         0.95
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.0%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelLeftBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.69
    SM Frequency            cycle/nsecond         1.35
    Elapsed Cycles                  cycle         4274
    Memory Throughput                   %         0.55
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.17
    L1/TEX Cache Throughput             %         3.35
    L2 Cache Throughput                 %         0.55
    SM Active Cycles                cycle       268.57
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelRightBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.31
    SM Frequency            cycle/nsecond         1.30
    Elapsed Cycles                  cycle         4341
    Memory Throughput                   %         0.54
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.33
    L1/TEX Cache Throughput             %         3.27
    L2 Cache Throughput                 %         0.54
    SM Active Cycles                cycle       275.10
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelBottomBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.43
    SM Frequency            cycle/nsecond         1.31
    Elapsed Cycles                  cycle         4366
    Memory Throughput                   %         0.64
    DRAM Throughput                     %         0.28
    Duration                      usecond         3.33
    L1/TEX Cache Throughput             %         3.32
    L2 Cache Throughput                 %         0.64
    SM Active Cycles                cycle       278.94
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.14
    Achieved Active Warps Per SM           warp         1.03
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelTopBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.53
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle         4350
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.29
    Duration                      usecond         3.26
    L1/TEX Cache Throughput             %         3.32
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle       278.71
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.97
    SM Frequency            cycle/nsecond         1.40
    Elapsed Cycles                  cycle        20073
    Memory Throughput                   %         2.63
    DRAM Throughput                     %         1.33
    Duration                      usecond        14.37
    L1/TEX Cache Throughput             %         2.66
    L2 Cache Throughput                 %         2.63
    SM Active Cycles                cycle     15296.68
    Compute (SM) Throughput             %        39.03
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           18
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.15
    Achieved Active Warps Per SM           warp         1.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (66.7%) and measured achieved occupancy (4.2%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (1, 8, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.60
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle        10991
    Memory Throughput                   %         0.91
    DRAM Throughput                     %         0.06
    Duration                      usecond         8.19
    L1/TEX Cache Throughput             %         0.91
    L2 Cache Throughput                 %         0.91
    SM Active Cycles                cycle      1057.38
    Compute (SM) Throughput             %         2.48
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 1, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.53
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle        11126
    Memory Throughput                   %         0.94
    DRAM Throughput                     %         0.11
    Duration                      usecond         8.38
    L1/TEX Cache Throughput             %         1.06
    L2 Cache Throughput                 %         0.94
    SM Active Cycles                cycle      1076.29
    Compute (SM) Throughput             %         2.49
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__reduce::ReduceAgent<thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>, thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>(T2, T3, T4, T5) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.85
    SM Frequency            cycle/nsecond         1.38
    Elapsed Cycles                  cycle         5839
    Memory Throughput                   %         2.45
    DRAM Throughput                     %         0.03
    Duration                      usecond         4.22
    L1/TEX Cache Throughput             %        18.56
    L2 Cache Throughput                 %         2.45
    SM Active Cycles                cycle        48.56
    Compute (SM) Throughput             %         0.13
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.07
    Achieved Active Warps Per SM           warp         6.76
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (14.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>, thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.88
    SM Frequency            cycle/nsecond         1.23
    Elapsed Cycles                  cycle         3159
    Memory Throughput                   %         0.66
    DRAM Throughput                     %         0.01
    Duration                      usecond         2.56
    L1/TEX Cache Throughput             %        44.64
    L2 Cache Throughput                 %         0.66
    SM Active Cycles                cycle        20.16
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        12.02
    Achieved Active Warps Per SM           warp         5.77
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>, thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.91
    SM Frequency            cycle/nsecond         1.54
    Elapsed Cycles                  cycle         2515
    Memory Throughput                   %         0.62
    DRAM Throughput                     %            0
    Duration                      usecond         1.63
    L1/TEX Cache Throughput             %       222.55
    L2 Cache Throughput                 %         0.62
    SM Active Cycles                cycle         4.04
    Compute (SM) Throughput             %         0.00
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.08
    Achieved Active Warps Per SM           warp         7.72
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (16.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::updateUnknownsKernel(const double *, const double *, const double *, const double *, const double *, const double *, const double *, const double *, double *, double *, double *, double, double, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.06
    SM Frequency            cycle/nsecond         1.25
    Elapsed Cycles                  cycle         5341
    Memory Throughput                   %        13.40
    DRAM Throughput                     %        13.40
    Duration                      usecond         4.26
    L1/TEX Cache Throughput             %         7.72
    L2 Cache Throughput                 %        11.00
    SM Active Cycles                cycle      3024.75
    Compute (SM) Throughput             %         5.08
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.10
    Achieved Active Warps Per SM           warp         1.97
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (66.7%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (66.7%) and measured achieved occupancy (4.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.20
    SM Frequency            cycle/nsecond         1.28
    Elapsed Cycles                  cycle         4004
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.14
    L1/TEX Cache Throughput             %        27.87
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        32.29
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.14
    Achieved Active Warps Per SM           warp         1.03
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.30
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         3979
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.02
    Duration                      usecond         3.07
    L1/TEX Cache Throughput             %        27.96
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        32.19
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.05
    Achieved Active Warps Per SM           warp         0.98
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.29
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         3935
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.04
    L1/TEX Cache Throughput             %        28.76
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.29
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelLeftBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.31
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         4228
    Memory Throughput                   %         0.56
    DRAM Throughput                     %         0.09
    Duration                      usecond         3.26
    L1/TEX Cache Throughput             %         3.34
    L2 Cache Throughput                 %         0.56
    SM Active Cycles                cycle       269.84
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelRightBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.08
    SM Frequency            cycle/nsecond         1.26
    Elapsed Cycles                  cycle         4226
    Memory Throughput                   %         0.56
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.36
    L1/TEX Cache Throughput             %         3.31
    L2 Cache Throughput                 %         0.56
    SM Active Cycles                cycle       272.19
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.04
    Achieved Active Warps Per SM           warp         0.98
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.0%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelBottomBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.63
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle         4381
    Memory Throughput                   %         0.64
    DRAM Throughput                     %         0.27
    Duration                      usecond         3.26
    L1/TEX Cache Throughput             %         3.32
    L2 Cache Throughput                 %         0.64
    SM Active Cycles                cycle       278.93
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelTopBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.43
    SM Frequency            cycle/nsecond         1.31
    Elapsed Cycles                  cycle         4371
    Memory Throughput                   %         0.72
    DRAM Throughput                     %         0.27
    Duration                      usecond         3.33
    L1/TEX Cache Throughput             %         3.32
    L2 Cache Throughput                 %         0.72
    SM Active Cycles                cycle       278.51
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.92
    SM Frequency            cycle/nsecond         1.39
    Elapsed Cycles                  cycle        20057
    Memory Throughput                   %         2.63
    DRAM Throughput                     %         1.33
    Duration                      usecond        14.40
    L1/TEX Cache Throughput             %         2.66
    L2 Cache Throughput                 %         2.63
    SM Active Cycles                cycle     15319.71
    Compute (SM) Throughput             %        39.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           18
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.14
    Achieved Active Warps Per SM           warp         1.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (66.7%) and measured achieved occupancy (4.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (1, 8, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.48
    SM Frequency            cycle/nsecond         1.32
    Elapsed Cycles                  cycle        10942
    Memory Throughput                   %         0.95
    DRAM Throughput                     %         0.05
    Duration                      usecond         8.29
    L1/TEX Cache Throughput             %         0.91
    L2 Cache Throughput                 %         0.95
    SM Active Cycles                cycle      1057.81
    Compute (SM) Throughput             %         2.49
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 1, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.58
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle        11148
    Memory Throughput                   %         0.94
    DRAM Throughput                     %         0.11
    Duration                      usecond         8.35
    L1/TEX Cache Throughput             %         1.06
    L2 Cache Throughput                 %         0.94
    SM Active Cycles                cycle      1075.03
    Compute (SM) Throughput             %         2.48
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__reduce::ReduceAgent<thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>, thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>(T2, T3, T4, T5) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.72
    SM Frequency            cycle/nsecond         1.36
    Elapsed Cycles                  cycle         5702
    Memory Throughput                   %         2.51
    DRAM Throughput                     %         0.03
    Duration                      usecond         4.19
    L1/TEX Cache Throughput             %        18.55
    L2 Cache Throughput                 %         2.51
    SM Active Cycles                cycle        48.60
    Compute (SM) Throughput             %         0.13
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.05
    Achieved Active Warps Per SM           warp         6.74
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (14.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>, thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.53
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle         3185
    Memory Throughput                   %         0.66
    DRAM Throughput                     %         0.02
    Duration                      usecond         2.40
    L1/TEX Cache Throughput             %        44.74
    L2 Cache Throughput                 %         0.66
    SM Active Cycles                cycle        20.12
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        11.86
    Achieved Active Warps Per SM           warp         5.69
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (11.9%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>, thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        11.61
    SM Frequency            cycle/nsecond         1.80
    Elapsed Cycles                  cycle         2933
    Memory Throughput                   %         0.53
    DRAM Throughput                     %            0
    Duration                      usecond         1.63
    L1/TEX Cache Throughput             %       208.16
    L2 Cache Throughput                 %         0.53
    SM Active Cycles                cycle         4.32
    Compute (SM) Throughput             %         0.00
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.06
    Achieved Active Warps Per SM           warp         7.71
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (16.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::updateUnknownsKernel(const double *, const double *, const double *, const double *, const double *, const double *, const double *, const double *, double *, double *, double *, double, double, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.67
    SM Frequency            cycle/nsecond         1.35
    Elapsed Cycles                  cycle         5976
    Memory Throughput                   %        12.00
    DRAM Throughput                     %        12.00
    Duration                      usecond         4.42
    L1/TEX Cache Throughput             %         7.71
    L2 Cache Throughput                 %         9.83
    SM Active Cycles                cycle      3028.75
    Compute (SM) Throughput             %         4.54
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.29
    Achieved Active Warps Per SM           warp         2.06
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (66.7%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (66.7%) and measured achieved occupancy (4.3%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.45
    SM Frequency            cycle/nsecond         1.32
    Elapsed Cycles                  cycle         4019
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.04
    L1/TEX Cache Throughput             %        27.41
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        32.84
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.10
    SM Frequency            cycle/nsecond         1.27
    Elapsed Cycles                  cycle         3983
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.14
    L1/TEX Cache Throughput             %        28.15
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.97
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.57
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle         3962
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.01
    Duration                      usecond         2.98
    L1/TEX Cache Throughput             %        28.71
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.35
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.10
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelLeftBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.30
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         4254
    Memory Throughput                   %         0.56
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.30
    L1/TEX Cache Throughput             %         3.33
    L2 Cache Throughput                 %         0.56
    SM Active Cycles                cycle       270.59
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelRightBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.56
    SM Frequency            cycle/nsecond         1.33
    Elapsed Cycles                  cycle         4342
    Memory Throughput                   %         0.54
    DRAM Throughput                     %         0.09
    Duration                      usecond         3.26
    L1/TEX Cache Throughput             %         3.29
    L2 Cache Throughput                 %         0.54
    SM Active Cycles                cycle       273.93
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelBottomBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.51
    SM Frequency            cycle/nsecond         1.32
    Elapsed Cycles                  cycle         4359
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.29
    Duration                      usecond         3.30
    L1/TEX Cache Throughput             %         3.28
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle       282.01
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.12
    Achieved Active Warps Per SM           warp         1.02
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelTopBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.43
    SM Frequency            cycle/nsecond         1.31
    Elapsed Cycles                  cycle         4372
    Memory Throughput                   %         0.64
    DRAM Throughput                     %         0.29
    Duration                      usecond         3.33
    L1/TEX Cache Throughput             %         3.22
    L2 Cache Throughput                 %         0.64
    SM Active Cycles                cycle       287.25
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.04
    Achieved Active Warps Per SM           warp         0.98
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.0%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.01
    SM Frequency            cycle/nsecond         1.40
    Elapsed Cycles                  cycle        20254
    Memory Throughput                   %         2.61
    DRAM Throughput                     %         1.32
    Duration                      usecond        14.43
    L1/TEX Cache Throughput             %         2.64
    L2 Cache Throughput                 %         2.61
    SM Active Cycles                cycle        15020
    Compute (SM) Throughput             %        38.12
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           18
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.15
    Achieved Active Warps Per SM           warp         1.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (66.7%) and measured achieved occupancy (4.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (1, 8, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.62
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle        11007
    Memory Throughput                   %         0.91
    DRAM Throughput                     %         0.06
    Duration                      usecond         8.19
    L1/TEX Cache Throughput             %         0.91
    L2 Cache Throughput                 %         0.91
    SM Active Cycles                cycle      1057.59
    Compute (SM) Throughput             %         2.48
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 1, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.65
    SM Frequency            cycle/nsecond         1.35
    Elapsed Cycles                  cycle        11181
    Memory Throughput                   %         0.94
    DRAM Throughput                     %         0.11
    Duration                      usecond         8.29
    L1/TEX Cache Throughput             %         1.06
    L2 Cache Throughput                 %         0.94
    SM Active Cycles                cycle      1073.72
    Compute (SM) Throughput             %         2.48
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__reduce::ReduceAgent<thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>, thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>(T2, T3, T4, T5) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.71
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle         5095
    Memory Throughput                   %         2.81
    DRAM Throughput                     %         0.03
    Duration                      usecond         4.22
    L1/TEX Cache Throughput             %        18.02
    L2 Cache Throughput                 %         2.81
    SM Active Cycles                cycle        50.01
    Compute (SM) Throughput             %         0.15
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        13.66
    Achieved Active Warps Per SM           warp         6.56
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (13.7%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>, thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.27
    SM Frequency            cycle/nsecond         1.28
    Elapsed Cycles                  cycle         3159
    Memory Throughput                   %         0.66
    DRAM Throughput                     %         0.02
    Duration                      usecond         2.46
    L1/TEX Cache Throughput             %        44.77
    L2 Cache Throughput                 %         0.66
    SM Active Cycles                cycle        20.10
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        12.03
    Achieved Active Warps Per SM           warp         5.77
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>, thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.54
    SM Frequency            cycle/nsecond         1.47
    Elapsed Cycles                  cycle         2495
    Memory Throughput                   %         0.63
    DRAM Throughput                     %            0
    Duration                      usecond         1.70
    L1/TEX Cache Throughput             %       222.55
    L2 Cache Throughput                 %         0.63
    SM Active Cycles                cycle         4.04
    Compute (SM) Throughput             %         0.00
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.08
    Achieved Active Warps Per SM           warp         7.72
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (16.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::updateUnknownsKernel(const double *, const double *, const double *, const double *, const double *, const double *, const double *, const double *, double *, double *, double *, double, double, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.53
    SM Frequency            cycle/nsecond         1.49
    Elapsed Cycles                  cycle         6187
    Memory Throughput                   %        11.60
    DRAM Throughput                     %        11.60
    Duration                      usecond         4.16
    L1/TEX Cache Throughput             %         7.41
    L2 Cache Throughput                 %         9.49
    SM Active Cycles                cycle      3148.53
    Compute (SM) Throughput             %         4.38
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.08
    Achieved Active Warps Per SM           warp         1.96
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (66.7%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (66.7%) and measured achieved occupancy (4.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.15
    SM Frequency            cycle/nsecond         1.26
    Elapsed Cycles                  cycle         4009
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.17
    L1/TEX Cache Throughput             %        27.70
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        32.49
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.12
    Achieved Active Warps Per SM           warp         1.02
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.41
    SM Frequency            cycle/nsecond         1.31
    Elapsed Cycles                  cycle         4070
    Memory Throughput                   %         0.46
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.10
    L1/TEX Cache Throughput             %        28.20
    L2 Cache Throughput                 %         0.46
    SM Active Cycles                cycle        31.91
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp            1
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelHdBufferEdges(double *, int, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.18
    SM Frequency            cycle/nsecond         1.27
    Elapsed Cycles                  cycle         3948
    Memory Throughput                   %         0.47
    DRAM Throughput                     %         0.01
    Duration                      usecond         3.10
    L1/TEX Cache Throughput             %        28.72
    L2 Cache Throughput                 %         0.47
    SM Active Cycles                cycle        31.34
    Compute (SM) Throughput             %         0.01
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.06
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelLeftBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.49
    SM Frequency            cycle/nsecond         1.32
    Elapsed Cycles                  cycle         4270
    Memory Throughput                   %         0.55
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.23
    L1/TEX Cache Throughput             %         3.36
    L2 Cache Throughput                 %         0.55
    SM Active Cycles                cycle       267.91
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelRightBoundary(double *, double *, double *, int, int, BoundaryType) (1, 8, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.75
    SM Frequency            cycle/nsecond         1.36
    Elapsed Cycles                  cycle         4431
    Memory Throughput                   %         0.53
    DRAM Throughput                     %         0.08
    Duration                      usecond         3.26
    L1/TEX Cache Throughput             %         3.30
    L2 Cache Throughput                 %         0.53
    SM Active Cycles                cycle       272.85
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelBottomBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.25
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle         4404
    Memory Throughput                   %         0.64
    DRAM Throughput                     %         0.28
    Duration                      usecond         3.42
    L1/TEX Cache Throughput             %         3.31
    L2 Cache Throughput                 %         0.64
    SM Active Cycles                cycle       279.37
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.09
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::kernelTopBoundary(double *, double *, double *, int, int, BoundaryType) (8, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.38
    SM Frequency            cycle/nsecond         1.30
    Elapsed Cycles                  cycle         4358
    Memory Throughput                   %         0.65
    DRAM Throughput                     %         0.27
    Duration                      usecond         3.36
    L1/TEX Cache Throughput             %         3.31
    L2 Cache Throughput                 %         0.65
    SM Active Cycles                cycle       279.13
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (33.3%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.98
    SM Frequency            cycle/nsecond         1.40
    Elapsed Cycles                  cycle        20072
    Memory Throughput                   %         2.63
    DRAM Throughput                     %         1.33
    Duration                      usecond        14.34
    L1/TEX Cache Throughput             %         2.66
    L2 Cache Throughput                 %         2.63
    SM Active Cycles                cycle     14552.25
    Compute (SM) Throughput             %        37.80
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           18
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         4.16
    Achieved Active Warps Per SM           warp         2.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (66.7%) and measured achieved occupancy (4.2%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (1, 8, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.62
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle        10949
    Memory Throughput                   %         0.91
    DRAM Throughput                     %         0.06
    Duration                      usecond         8.16
    L1/TEX Cache Throughput             %         0.96
    L2 Cache Throughput                 %         0.91
    SM Active Cycles                cycle       996.74
    Compute (SM) Throughput             %         2.35
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::computeNetUpdatesKernel(const double *, const double *, const double *, const double *, double *, double *, double *, double *, double *, double *, double *, double *, double *, int, int, int, int, int, int) (8, 1, 1)x(1, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.62
    SM Frequency            cycle/nsecond         1.34
    Elapsed Cycles                  cycle        11122
    Memory Throughput                   %         0.94
    DRAM Throughput                     %         0.11
    Duration                      usecond         8.29
    L1/TEX Cache Throughput             %         1.12
    L2 Cache Throughput                 %         0.94
    SM Active Cycles                cycle      1016.74
    Compute (SM) Throughput             %         2.35
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              54
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             512
    Threads                                   thread              64
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that can fit on the SM The     
          difference between calculated theoretical (33.3%) and measured achieved occupancy (2.1%) can be the result    
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__reduce::ReduceAgent<thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>, thrust::zip_iterator<thrust::tuple<thrust::device_ptr<double>, thrust::cuda_cub::counting_iterator_t<long>, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>>, thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, int, thrust::cuda_cub::__extrema::arg_max_f<double, long, thrust::less<double>>>(T2, T3, T4, T5) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.86
    SM Frequency            cycle/nsecond         1.38
    Elapsed Cycles                  cycle         5746
    Memory Throughput                   %         2.49
    DRAM Throughput                     %         0.03
    Duration                      usecond         4.16
    L1/TEX Cache Throughput             %        18.62
    L2 Cache Throughput                 %         2.49
    SM Active Cycles                cycle        48.43
    Compute (SM) Throughput             %         0.13
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block             160
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        14.19
    Achieved Active Warps Per SM           warp         6.81
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (14.2%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>, thrust::cuda_cub::__uninitialized_copy::functor<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type> *, thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.10
    SM Frequency            cycle/nsecond         1.26
    Elapsed Cycles                  cycle         3189
    Memory Throughput                   %         0.66
    DRAM Throughput                     %         0.01
    Duration                      usecond         2.53
    L1/TEX Cache Throughput             %        45.13
    L2 Cache Throughput                 %         0.66
    SM Active Cycles                cycle        19.94
    Compute (SM) Throughput             %         0.04
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        12.23
    Achieved Active Warps Per SM           warp         5.87
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (12.2%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>, thrust::cuda_cub::for_each_f<thrust::pointer<thrust::tuple<double, long, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type, thrust::null_type>, thrust::cuda_cub::tag, thrust::use_default, thrust::use_default>, thrust::detail::wrapped_function<thrust::detail::allocator_traits_detail::gozer, void>>, long>(T2, T3) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.79
    SM Frequency            cycle/nsecond         1.68
    Elapsed Cycles                  cycle         2908
    Memory Throughput                   %         0.54
    DRAM Throughput                     %            0
    Duration                      usecond         1.73
    L1/TEX Cache Throughput             %       207.46
    L2 Cache Throughput                 %         0.54
    SM Active Cycles                cycle         4.34
    Compute (SM) Throughput             %         0.00
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             256
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.07
    Achieved Active Warps Per SM           warp         7.71
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (16.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  cuda::updateUnknownsKernel(const double *, const double *, const double *, const double *, const double *, const double *, const double *, const double *, double *, double *, double *, double, double, int, int) (8, 8, 1)x(8, 8, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.91
    SM Frequency            cycle/nsecond         1.38
    Elapsed Cycles                  cycle         5590
    Memory Throughput                   %        12.80
    DRAM Throughput                     %        12.80
    Duration                      usecond         4.03
    L1/TEX Cache Throughput             %         7.37
    L2 Cache Throughput                 %        10.50
    SM Active Cycles                cycle      3167.53
    Compute (SM) Throughput             %         4.86
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 68             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %         3.93
    Achieved Active Warps Per SM           warp         1.89
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (66.7%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (66.7%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (66.7%) and measured achieved occupancy (3.9%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

